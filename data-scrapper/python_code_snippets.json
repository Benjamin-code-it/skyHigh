[
    {
        "repository": "psf/requests",
        "file_name": "setup.py",
        "content": "#!/usr/bin/env python\nimport os\nimport sys\nfrom codecs import open\n\nfrom setuptools import setup\nfrom setuptools.command.test import test as TestCommand\n\nCURRENT_PYTHON = sys.version_info[:2]\nREQUIRED_PYTHON = (3, 7)\n\nif CURRENT_PYTHON < REQUIRED_PYTHON:\n    sys.stderr.write(\n        \"\"\"\n==========================\nUnsupported Python version\n==========================\nThis version of Requests requires at least Python {}.{}, but\nyou're trying to install it on Python {}.{}. To resolve this,\nconsider upgrading to a supported Python version.\n\nIf you can't upgrade your Python version, you'll need to\npin to an older version of Requests (<2.28).\n\"\"\".format(\n            *(REQUIRED_PYTHON + CURRENT_PYTHON)\n        )\n    )\n    sys.exit(1)\n\n\nclass PyTest(TestCommand):\n    user_options = [(\"pytest-args=\", \"a\", \"Arguments to pass into py.test\")]\n\n    def initialize_options(self):\n        TestCommand.initialize_options(self)\n        try:\n            from multiprocessing import cpu_count\n\n            self.pytest_args = [\"-n\", str(cpu_count()), \"--boxed\"]\n        except (ImportError, NotImplementedError):\n            self.pytest_args = [\"-n\", \"1\", \"--boxed\"]\n\n    def finalize_options(self):\n        TestCommand.finalize_options(self)\n        self.test_args = []\n        self.test_suite = True\n\n    def run_tests(self):\n        import pytest\n\n        errno = pytest.main(self.pytest_args)\n        sys.exit(errno)\n\n\n# 'setup.py publish' shortcut.\nif sys.argv[-1] == \"publish\":\n    os.system(\"python setup.py sdist bdist_wheel\")\n    os.system(\"twine upload dist/*\")\n    sys.exit()\n\nrequires = [\n    \"charset_normalizer>=2,<4\",\n    \"idna>=2.5,<4\",\n    \"urllib3>=1.21.1,<3\",\n    \"certifi>=2017.4.17\",\n]\ntest_requirements = [\n    \"pytest-httpbin==2.0.0\",\n    \"pytest-cov\",\n    \"pytest-mock\",\n    \"pytest-xdist\",\n    \"PySocks>=1.5.6, !=1.5.7\",\n    \"pytest>=3\",\n]\n\nabout = {}\nhere = os.path.abspath(os.path.dirname(__file__))\nwith open(os.path.join(here, \"src\", \"requests\", \"__version__.py\"), \"r\", \"utf-8\") as f:\n    exec(f.read(), about)\n\nwith open(\"README.md\", \"r\", \"utf-8\") as f:\n    readme = f.read()\n\nsetup(\n    name=about[\"__title__\"],\n    version=about[\"__version__\"],\n    description=about[\"__description__\"],\n    long_description=readme,\n    long_description_content_type=\"text/markdown\",\n    author=about[\"__author__\"],\n    author_email=about[\"__author_email__\"],\n    url=about[\"__url__\"],\n    packages=[\"requests\"],\n    package_data={\"\": [\"LICENSE\", \"NOTICE\"]},\n    package_dir={\"\": \"src\"},\n    include_package_data=True,\n    python_requires=\">=3.7\",\n    install_requires=requires,\n    license=about[\"__license__\"],\n    zip_safe=False,\n    classifiers=[\n        \"Development Status :: 5 - Production/Stable\",\n        \"Environment :: Web Environment\",\n        \"Intended Audience :: Developers\",\n        \"License :: OSI Approved :: Apache Software License\",\n        \"Natural Language :: English\",\n        \"Operating System :: OS Independent\",\n        \"Programming Language :: Python\",\n        \"Programming Language :: Python :: 3\",\n        \"Programming Language :: Python :: 3.7\",\n        \"Programming Language :: Python :: 3.8\",\n        \"Programming Language :: Python :: 3.9\",\n        \"Programming Language :: Python :: 3.10\",\n        \"Programming Language :: Python :: 3.11\",\n        \"Programming Language :: Python :: 3.12\",\n        \"Programming Language :: Python :: 3 :: Only\",\n        \"Programming Language :: Python :: Implementation :: CPython\",\n        \"Programming Language :: Python :: Implementation :: PyPy\",\n        \"Topic :: Internet :: WWW/HTTP\",\n        \"Topic :: Software Development :: Libraries\",\n    ],\n    cmdclass={\"test\": PyTest},\n    tests_require=test_requirements,\n    extras_require={\n        \"security\": [],\n        \"socks\": [\"PySocks>=1.5.6, !=1.5.7\"],\n        \"use_chardet_on_py3\": [\"chardet>=3.0.2,<6\"],\n    },\n    project_urls={\n        \"Documentation\": \"https://requests.readthedocs.io\",\n        \"Source\": \"https://github.com/psf/requests\",\n    },\n)\n"
    },
    {
        "repository": "numpy/numpy",
        "file_name": "pavement.py",
        "content": "r\"\"\"\nThis paver file is intended to help with the release process as much as\npossible. It relies on virtualenv to generate 'bootstrap' environments as\nindependent from the user system as possible (e.g. to make sure the sphinx doc\nis built against the built numpy, not an installed one).\n\nBuilding changelog + notes\n==========================\n\nAssumes you have git and the binaries/tarballs in installers/::\n\n    paver write_release\n    paver write_note\n\nThis automatically put the checksum into README.rst, and writes the Changelog.\n\nTODO\n====\n    - the script is messy, lots of global variables\n    - make it more easily customizable (through command line args)\n    - missing targets: install & test, sdist test, debian packaging\n    - fix bdist_mpkg: we build the same source twice -> how to make sure we use\n      the same underlying python for egg install in venv and for bdist_mpkg\n\"\"\"\nimport os\nimport sys\nimport shutil\nimport hashlib\nimport textwrap\n\n# The paver package needs to be installed to run tasks\nimport paver\nfrom paver.easy import Bunch, options, task, sh\n\n\n#-----------------------------------\n# Things to be changed for a release\n#-----------------------------------\n\n# Path to the release notes\nRELEASE_NOTES = 'doc/source/release/2.0.0-notes.rst'\n\n\n#-------------------------------------------------------\n# Hardcoded build/install dirs, virtualenv options, etc.\n#-------------------------------------------------------\n\n# Where to put the release installers\noptions(installers=Bunch(releasedir=\"release\",\n                         installersdir=os.path.join(\"release\", \"installers\")),)\n\n\n#------------------------\n# Get the release version\n#------------------------\n\nsys.path.insert(0, os.path.dirname(__file__))\ntry:\n    from setup import FULLVERSION\nfinally:\n    sys.path.pop(0)\n\n\n#--------------------------\n# Source distribution stuff\n#--------------------------\ndef tarball_name(ftype='gztar'):\n    \"\"\"Generate source distribution name\n\n    Parameters\n    ----------\n    ftype : {'zip', 'gztar'}\n        Type of archive, default is 'gztar'.\n\n    \"\"\"\n    root = f'numpy-{FULLVERSION}'\n    if ftype == 'gztar':\n        return root + '.tar.gz'\n    elif ftype == 'zip':\n        return root + '.zip'\n    raise ValueError(f\"Unknown type {type}\")\n\n\n@task\ndef sdist(options):\n    \"\"\"Make source distributions.\n\n    Parameters\n    ----------\n    options :\n        Set by ``task`` decorator.\n\n    \"\"\"\n    # First clean the repo and update submodules (for up-to-date doc html theme\n    # and Sphinx extensions)\n    sh('git clean -xdf')\n    sh('git submodule init')\n    sh('git submodule update')\n\n    # To be sure to bypass paver when building sdist... paver + numpy.distutils\n    # do not play well together.\n    # Cython is run over all Cython files in setup.py, so generated C files\n    # will be included.\n    sh('python3 setup.py sdist --formats=gztar,zip')\n\n    # Copy the superpack into installers dir\n    idirs = options.installers.installersdir\n    if not os.path.exists(idirs):\n        os.makedirs(idirs)\n\n    for ftype in ['gztar', 'zip']:\n        source = os.path.join('dist', tarball_name(ftype))\n        target = os.path.join(idirs, tarball_name(ftype))\n        shutil.copy(source, target)\n\n\n#-------------\n# README stuff\n#-------------\n\ndef _compute_hash(idirs, hashfunc):\n    \"\"\"Hash files using given hashfunc.\n\n    Parameters\n    ----------\n    idirs : directory path\n        Directory containing files to be hashed.\n    hashfunc : hash function\n        Function to be used to hash the files.\n\n    \"\"\"\n    released = paver.path.path(idirs).listdir()\n    checksums = []\n    for fpath in sorted(released):\n        with open(fpath, 'rb') as fin:\n            fhash = hashfunc(fin.read())\n            checksums.append(\n                '%s  %s' % (fhash.hexdigest(), os.path.basename(fpath)))\n    return checksums\n\n\ndef compute_md5(idirs):\n    \"\"\"Compute md5 hash of files in idirs.\n\n    Parameters\n    ----------\n    idirs : directory path\n        Directory containing files to be hashed.\n\n    \"\"\"\n    return _compute_hash(idirs, hashlib.md5)\n\n\ndef compute_sha256(idirs):\n    \"\"\"Compute sha256 hash of files in idirs.\n\n    Parameters\n    ----------\n    idirs : directory path\n        Directory containing files to be hashed.\n\n    \"\"\"\n    # better checksum so gpg signed README.rst containing the sums can be used\n    # to verify the binaries instead of signing all binaries\n    return _compute_hash(idirs, hashlib.sha256)\n\n\ndef write_release_task(options, filename='README'):\n    \"\"\"Append hashes of release files to release notes.\n\n    This appends file hashes to the release notes and creates\n    four README files of the result in various formats:\n\n    - README.rst\n    - README.rst.gpg\n    - README.md\n    - README.md.gpg\n\n    The md file are created using `pandoc` so that the links are\n    properly updated. The gpg files are kept separate, so that\n    the unsigned files may be edited before signing if needed.\n\n    Parameters\n    ----------\n    options :\n        Set by ``task`` decorator.\n    filename : str\n        Filename of the modified notes. The file is written\n        in the release directory.\n\n    \"\"\"\n    idirs = options.installers.installersdir\n    notes = paver.path.path(RELEASE_NOTES)\n    rst_readme = paver.path.path(filename + '.rst')\n    md_readme = paver.path.path(filename + '.md')\n\n    # append hashes\n    with open(rst_readme, 'w') as freadme:\n        with open(notes) as fnotes:\n            freadme.write(fnotes.read())\n\n        freadme.writelines(textwrap.dedent(\n            \"\"\"\n            Checksums\n            =========\n\n            MD5\n            ---\n            ::\n\n            \"\"\"))\n        freadme.writelines([f'    {c}\\n' for c in compute_md5(idirs)])\n\n        freadme.writelines(textwrap.dedent(\n            \"\"\"\n            SHA256\n            ------\n            ::\n\n            \"\"\"))\n        freadme.writelines([f'    {c}\\n' for c in compute_sha256(idirs)])\n\n    # generate md file using pandoc before signing\n    sh(f\"pandoc -s -o {md_readme} {rst_readme}\")\n\n    # Sign files\n    if hasattr(options, 'gpg_key'):\n        cmd = f'gpg --clearsign --armor --default_key {options.gpg_key}'\n    else:\n        cmd = 'gpg --clearsign --armor'\n\n    sh(cmd + f' --output {rst_readme}.gpg {rst_readme}')\n    sh(cmd + f' --output {md_readme}.gpg {md_readme}')\n\n\n@task\ndef write_release(options):\n    \"\"\"Write the README files.\n\n    Two README files are generated from the release notes, one in ``rst``\n    markup for the general release, the other in ``md`` markup for the github\n    release notes.\n\n    Parameters\n    ----------\n    options :\n        Set by ``task`` decorator.\n\n    \"\"\"\n    rdir = options.installers.releasedir\n    write_release_task(options, os.path.join(rdir, 'README'))\n"
    },
    {
        "repository": "django/django",
        "file_name": "setup.py",
        "content": "import os\nimport site\nimport sys\nfrom distutils.sysconfig import get_python_lib\n\nfrom setuptools import setup\n\n# Allow editable install into user site directory.\n# See https://github.com/pypa/pip/issues/7953.\nsite.ENABLE_USER_SITE = \"--user\" in sys.argv[1:]\n\n# Warn if we are installing over top of an existing installation. This can\n# cause issues where files that were deleted from a more recent Django are\n# still present in site-packages. See #18115.\noverlay_warning = False\nif \"install\" in sys.argv:\n    lib_paths = [get_python_lib()]\n    if lib_paths[0].startswith(\"/usr/lib/\"):\n        # We have to try also with an explicit prefix of /usr/local in order to\n        # catch Debian's custom user site-packages directory.\n        lib_paths.append(get_python_lib(prefix=\"/usr/local\"))\n    for lib_path in lib_paths:\n        existing_path = os.path.abspath(os.path.join(lib_path, \"django\"))\n        if os.path.exists(existing_path):\n            # We note the need for the warning here, but present it after the\n            # command is run, so it's more likely to be seen.\n            overlay_warning = True\n            break\n\n\nsetup()\n\n\nif overlay_warning:\n    sys.stderr.write(\n        \"\"\"\n\n========\nWARNING!\n========\n\nYou have just installed Django over top of an existing\ninstallation, without removing it first. Because of this,\nyour install may now include extraneous files from a\nprevious version that have since been removed from\nDjango. This is known to cause a variety of problems. You\nshould manually remove the\n\n%(existing_path)s\n\ndirectory and re-install Django.\n\n\"\"\"\n        % {\"existing_path\": existing_path}\n    )\n"
    },
    {
        "repository": "pandas-dev/pandas",
        "file_name": "generate_pxi.py",
        "content": "import argparse\nimport os\n\nfrom Cython import Tempita\n\n\ndef process_tempita(pxifile, outfile):\n    with open(pxifile, encoding=\"utf-8\") as f:\n        tmpl = f.read()\n    pyxcontent = Tempita.sub(tmpl)\n\n    with open(outfile, \"w\", encoding=\"utf-8\") as f:\n        f.write(pyxcontent)\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"infile\", type=str, help=\"Path to the input file\")\n    parser.add_argument(\"-o\", \"--outdir\", type=str, help=\"Path to the output directory\")\n    args = parser.parse_args()\n\n    if not args.infile.endswith(\".in\"):\n        raise ValueError(f\"Unexpected extension: {args.infile}\")\n\n    outdir_abs = os.path.join(os.getcwd(), args.outdir)\n    outfile = os.path.join(\n        outdir_abs, os.path.splitext(os.path.split(args.infile)[1])[0]\n    )\n\n    process_tempita(args.infile, outfile)\n\n\nmain()\n"
    },
    {
        "repository": "pandas-dev/pandas",
        "file_name": "generate_version.py",
        "content": "#!/usr/bin/env python3\n\n# Note: This file has to live next to setup.py or versioneer will not work\nimport argparse\nimport os\nimport sys\n\nimport versioneer\n\nsys.path.insert(0, \"\")\n\n\ndef write_version_info(path):\n    version = None\n    git_version = None\n\n    try:\n        import _version_meson\n\n        version = _version_meson.__version__\n        git_version = _version_meson.__git_version__\n    except ImportError:\n        version = versioneer.get_version()\n        git_version = versioneer.get_versions()[\"full-revisionid\"]\n    if os.environ.get(\"MESON_DIST_ROOT\"):\n        path = os.path.join(os.environ.get(\"MESON_DIST_ROOT\"), path)\n    with open(path, \"w\", encoding=\"utf-8\") as file:\n        file.write(f'__version__=\"{version}\"\\n')\n        file.write(f'__git_version__=\"{git_version}\"\\n')\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"-o\",\n        \"--outfile\",\n        type=str,\n        help=\"Path to write version info to\",\n        required=False,\n    )\n    parser.add_argument(\n        \"--print\",\n        default=False,\n        action=\"store_true\",\n        help=\"Whether to print out the version\",\n        required=False,\n    )\n    args = parser.parse_args()\n\n    if args.outfile:\n        if not args.outfile.endswith(\".py\"):\n            raise ValueError(\n                f\"Output file must be a Python file. \"\n                f\"Got: {args.outfile} as filename instead\"\n            )\n\n        write_version_info(args.outfile)\n\n    if args.print:\n        try:\n            import _version_meson\n\n            version = _version_meson.__version__\n        except ImportError:\n            version = versioneer.get_version()\n        print(version)\n\n\nmain()\n"
    },
    {
        "repository": "pandas-dev/pandas",
        "file_name": "setup.py",
        "content": "#!/usr/bin/env python3\n\n\"\"\"\nParts of this file were taken from the pyzmq project\n(https://github.com/zeromq/pyzmq) which have been permitted for use under the\nBSD license. Parts are from lxml (https://github.com/lxml/lxml)\n\"\"\"\n\nimport argparse\nimport multiprocessing\nimport os\nfrom os.path import join as pjoin\nimport platform\nimport shutil\nimport sys\nfrom sysconfig import get_config_vars\n\nimport numpy\nfrom pkg_resources import parse_version\nfrom setuptools import (\n    Command,\n    Extension,\n    setup,\n)\nfrom setuptools.command.build_ext import build_ext as _build_ext\nimport versioneer\n\ncmdclass = versioneer.get_cmdclass()\n\n\ndef is_platform_windows():\n    return sys.platform in (\"win32\", \"cygwin\")\n\n\ndef is_platform_mac():\n    return sys.platform == \"darwin\"\n\n\n# note: sync with pyproject.toml, environment.yml and asv.conf.json\nmin_cython_ver = \"0.29.33\"\n\ntry:\n    from Cython import (\n        Tempita,\n        __version__ as _CYTHON_VERSION,\n    )\n    from Cython.Build import cythonize\n\n    _CYTHON_INSTALLED = parse_version(_CYTHON_VERSION) >= parse_version(min_cython_ver)\nexcept ImportError:\n    _CYTHON_VERSION = None\n    _CYTHON_INSTALLED = False\n    cythonize = lambda x, *args, **kwargs: x  # dummy func\n\n\n_pxi_dep_template = {\n    \"algos\": [\"_libs/algos_common_helper.pxi.in\", \"_libs/algos_take_helper.pxi.in\"],\n    \"hashtable\": [\n        \"_libs/hashtable_class_helper.pxi.in\",\n        \"_libs/hashtable_func_helper.pxi.in\",\n        \"_libs/khash_for_primitive_helper.pxi.in\",\n    ],\n    \"index\": [\"_libs/index_class_helper.pxi.in\"],\n    \"sparse\": [\"_libs/sparse_op_helper.pxi.in\"],\n    \"interval\": [\"_libs/intervaltree.pxi.in\"],\n}\n\n_pxifiles = []\n_pxi_dep = {}\nfor module, files in _pxi_dep_template.items():\n    pxi_files = [pjoin(\"pandas\", x) for x in files]\n    _pxifiles.extend(pxi_files)\n    _pxi_dep[module] = pxi_files\n\n\nclass build_ext(_build_ext):\n    @classmethod\n    def render_templates(cls, pxifiles):\n        for pxifile in pxifiles:\n            # build pxifiles first, template extension must be .pxi.in\n            assert pxifile.endswith(\".pxi.in\")\n            outfile = pxifile[:-3]\n\n            if (\n                os.path.exists(outfile)\n                and os.stat(pxifile).st_mtime < os.stat(outfile).st_mtime\n            ):\n                # if .pxi.in is not updated, no need to output .pxi\n                continue\n\n            with open(pxifile, encoding=\"utf-8\") as f:\n                tmpl = f.read()\n            pyxcontent = Tempita.sub(tmpl)\n\n            with open(outfile, \"w\", encoding=\"utf-8\") as f:\n                f.write(pyxcontent)\n\n    def build_extensions(self):\n        # if building from c files, don't need to\n        # generate template output\n        if _CYTHON_INSTALLED:\n            self.render_templates(_pxifiles)\n\n        super().build_extensions()\n\n\nclass CleanCommand(Command):\n    \"\"\"Custom command to clean the .so and .pyc files.\"\"\"\n\n    user_options = [(\"all\", \"a\", \"\")]\n\n    def initialize_options(self):\n        self.all = True\n        self._clean_me = []\n        self._clean_trees = []\n\n        base = pjoin(\"pandas\", \"_libs\", \"src\")\n        parser = pjoin(base, \"parser\")\n        vendored = pjoin(base, \"vendored\")\n        dt = pjoin(base, \"datetime\")\n        ujson_python = pjoin(vendored, \"ujson\", \"python\")\n        ujson_lib = pjoin(vendored, \"ujson\", \"lib\")\n        self._clean_exclude = [\n            pjoin(vendored, \"numpy\", \"datetime\", \"np_datetime.c\"),\n            pjoin(vendored, \"numpy\", \"datetime\", \"np_datetime_strings.c\"),\n            pjoin(dt, \"date_conversions.c\"),\n            pjoin(parser, \"tokenizer.c\"),\n            pjoin(parser, \"io.c\"),\n            pjoin(ujson_python, \"ujson.c\"),\n            pjoin(ujson_python, \"objToJSON.c\"),\n            pjoin(ujson_python, \"JSONtoObj.c\"),\n            pjoin(ujson_lib, \"ultrajsonenc.c\"),\n            pjoin(ujson_lib, \"ultrajsondec.c\"),\n            pjoin(dt, \"pd_datetime.c\"),\n            pjoin(parser, \"pd_parser.c\"),\n        ]\n\n        for root, dirs, files in os.walk(\"pandas\"):\n            for f in files:\n                filepath = pjoin(root, f)\n                if filepath in self._clean_exclude:\n                    continue\n\n                if os.path.splitext(f)[-1] in (\n                    \".pyc\",\n                    \".so\",\n                    \".o\",\n                    \".pyo\",\n                    \".pyd\",\n                    \".c\",\n                    \".cpp\",\n                    \".orig\",\n                ):\n                    self._clean_me.append(filepath)\n            self._clean_trees.append(pjoin(root, d) for d in dirs if d == \"__pycache__\")\n\n        # clean the generated pxi files\n        for pxifile in _pxifiles:\n            pxifile_replaced = pxifile.replace(\".pxi.in\", \".pxi\")\n            self._clean_me.append(pxifile_replaced)\n\n        self._clean_trees.append(d for d in (\"build\", \"dist\") if os.path.exists(d))\n\n    def finalize_options(self):\n        pass\n\n    def run(self):\n        for clean_me in self._clean_me:\n            try:\n                os.unlink(clean_me)\n            except OSError:\n                pass\n        for clean_tree in self._clean_trees:\n            try:\n                shutil.rmtree(clean_tree)\n            except OSError:\n                pass\n\n\n# we need to inherit from the versioneer\n# class as it encodes the version info\nsdist_class = cmdclass[\"sdist\"]\n\n\nclass CheckSDist(sdist_class):\n    \"\"\"Custom sdist that ensures Cython has compiled all pyx files to c.\"\"\"\n\n    _pyxfiles = [\n        \"pandas/_libs/arrays.pyx\",\n        \"pandas/_libs/lib.pyx\",\n        \"pandas/_libs/hashtable.pyx\",\n        \"pandas/_libs/tslib.pyx\",\n        \"pandas/_libs/index.pyx\",\n        \"pandas/_libs/internals.pyx\",\n        \"pandas/_libs/algos.pyx\",\n        \"pandas/_libs/join.pyx\",\n        \"pandas/_libs/indexing.pyx\",\n        \"pandas/_libs/interval.pyx\",\n        \"pandas/_libs/hashing.pyx\",\n        \"pandas/_libs/missing.pyx\",\n        \"pandas/_libs/testing.pyx\",\n        \"pandas/_libs/sparse.pyx\",\n        \"pandas/_libs/ops.pyx\",\n        \"pandas/_libs/parsers.pyx\",\n        \"pandas/_libs/tslibs/base.pyx\",\n        \"pandas/_libs/tslibs/ccalendar.pyx\",\n        \"pandas/_libs/tslibs/dtypes.pyx\",\n        \"pandas/_libs/tslibs/period.pyx\",\n        \"pandas/_libs/tslibs/strptime.pyx\",\n        \"pandas/_libs/tslibs/np_datetime.pyx\",\n        \"pandas/_libs/tslibs/timedeltas.pyx\",\n        \"pandas/_libs/tslibs/timestamps.pyx\",\n        \"pandas/_libs/tslibs/timezones.pyx\",\n        \"pandas/_libs/tslibs/conversion.pyx\",\n        \"pandas/_libs/tslibs/fields.pyx\",\n        \"pandas/_libs/tslibs/offsets.pyx\",\n        \"pandas/_libs/tslibs/parsing.pyx\",\n        \"pandas/_libs/tslibs/tzconversion.pyx\",\n        \"pandas/_libs/tslibs/vectorized.pyx\",\n        \"pandas/_libs/window/indexers.pyx\",\n        \"pandas/_libs/writers.pyx\",\n        \"pandas/_libs/sas.pyx\",\n        \"pandas/_libs/byteswap.pyx\",\n    ]\n\n    _cpp_pyxfiles = [\n        \"pandas/_libs/window/aggregations.pyx\",\n    ]\n\n    def initialize_options(self):\n        sdist_class.initialize_options(self)\n\n    def run(self):\n        if \"cython\" in cmdclass:\n            self.run_command(\"cython\")\n        else:\n            # If we are not running cython then\n            # compile the extensions correctly\n            pyx_files = [(self._pyxfiles, \"c\"), (self._cpp_pyxfiles, \"cpp\")]\n\n            for pyxfiles, extension in pyx_files:\n                for pyxfile in pyxfiles:\n                    sourcefile = pyxfile[:-3] + extension\n                    msg = (\n                        f\"{extension}-source file '{sourcefile}' not found.\\n\"\n                        \"Run 'setup.py cython' before sdist.\"\n                    )\n                    assert os.path.isfile(sourcefile), msg\n        sdist_class.run(self)\n\n\nclass CheckingBuildExt(build_ext):\n    \"\"\"\n    Subclass build_ext to get clearer report if Cython is necessary.\n    \"\"\"\n\n    def check_cython_extensions(self, extensions):\n        for ext in extensions:\n            for src in ext.sources:\n                if not os.path.exists(src):\n                    print(f\"{ext.name}: -> [{ext.sources}]\")\n                    raise Exception(\n                        f\"\"\"Cython-generated file '{src}' not found.\n                Cython is required to compile pandas from a development branch.\n                Please install Cython or download a release package of pandas.\n                \"\"\"\n                    )\n\n    def build_extensions(self):\n        self.check_cython_extensions(self.extensions)\n        build_ext.build_extensions(self)\n\n\nclass CythonCommand(build_ext):\n    \"\"\"\n    Custom command subclassed from Cython.Distutils.build_ext\n    to compile pyx->c, and stop there. All this does is override the\n    C-compile method build_extension() with a no-op.\n    \"\"\"\n\n    def build_extension(self, ext):\n        pass\n\n\nclass DummyBuildSrc(Command):\n    \"\"\"numpy's build_src command interferes with Cython's build_ext.\"\"\"\n\n    user_options = []\n\n    def initialize_options(self):\n        self.py_modules_dict = {}\n\n    def finalize_options(self):\n        pass\n\n    def run(self):\n        pass\n\n\ncmdclass[\"clean\"] = CleanCommand\ncmdclass[\"build_ext\"] = CheckingBuildExt\n\nif _CYTHON_INSTALLED:\n    suffix = \".pyx\"\n    cmdclass[\"cython\"] = CythonCommand\nelse:\n    suffix = \".c\"\n    cmdclass[\"build_src\"] = DummyBuildSrc\n\n# ----------------------------------------------------------------------\n# Preparation of compiler arguments\n\ndebugging_symbols_requested = \"--with-debugging-symbols\" in sys.argv\nif debugging_symbols_requested:\n    sys.argv.remove(\"--with-debugging-symbols\")\n\n\nif sys.byteorder == \"big\":\n    endian_macro = [(\"__BIG_ENDIAN__\", \"1\")]\nelse:\n    endian_macro = [(\"__LITTLE_ENDIAN__\", \"1\")]\n\n\nextra_compile_args = []\nextra_link_args = []\nif is_platform_windows():\n    if debugging_symbols_requested:\n        extra_compile_args.append(\"/Z7\")\n        extra_link_args.append(\"/DEBUG\")\nelse:\n    # PANDAS_CI=1 is set in CI\n    if os.environ.get(\"PANDAS_CI\", \"0\") == \"1\":\n        extra_compile_args.append(\"-Werror\")\n    if debugging_symbols_requested:\n        extra_compile_args.append(\"-g3\")\n        extra_compile_args.append(\"-UNDEBUG\")\n        extra_compile_args.append(\"-O0\")\n\n# Build for at least macOS 10.9 when compiling on a 10.9 system or above,\n# overriding CPython distuitls behaviour which is to target the version that\n# python was built for. This may be overridden by setting\n# MACOSX_DEPLOYMENT_TARGET before calling setup.py\nif is_platform_mac():\n    if \"MACOSX_DEPLOYMENT_TARGET\" not in os.environ:\n        current_system = platform.mac_ver()[0]\n        python_target = get_config_vars().get(\n            \"MACOSX_DEPLOYMENT_TARGET\", current_system\n        )\n        target_macos_version = \"10.9\"\n        parsed_macos_version = parse_version(target_macos_version)\n        if (\n            parse_version(str(python_target))\n            < parsed_macos_version\n            <= parse_version(current_system)\n        ):\n            os.environ[\"MACOSX_DEPLOYMENT_TARGET\"] = target_macos_version\n\n    if sys.version_info[:2] == (3, 8):  # GH 33239\n        extra_compile_args.append(\"-Wno-error=deprecated-declarations\")\n\n    # https://github.com/pandas-dev/pandas/issues/35559\n    extra_compile_args.append(\"-Wno-error=unreachable-code\")\n\n# enable coverage by building cython files by setting the environment variable\n# \"PANDAS_CYTHON_COVERAGE\" (with a Truthy value) or by running build_ext\n# with `--with-cython-coverage`enabled\nlinetrace = os.environ.get(\"PANDAS_CYTHON_COVERAGE\", False)\nif \"--with-cython-coverage\" in sys.argv:\n    linetrace = True\n    sys.argv.remove(\"--with-cython-coverage\")\n\n# Note: if not using `cythonize`, coverage can be enabled by\n# pinning `ext.cython_directives = directives` to each ext in extensions.\n# github.com/cython/cython/wiki/enhancements-compilerdirectives#in-setuppy\ndirectives = {\"linetrace\": False, \"language_level\": 3, \"always_allow_keywords\": True}\nmacros = []\nif linetrace:\n    # https://pypkg.com/pypi/pytest-cython/f/tests/example-project/setup.py\n    directives[\"linetrace\"] = True\n    macros = [(\"CYTHON_TRACE\", \"1\"), (\"CYTHON_TRACE_NOGIL\", \"1\")]\n\n# silence build warnings about deprecated API usage\n# we can't do anything about these warnings because they stem from\n# cython+numpy version mismatches.\nmacros.append((\"NPY_NO_DEPRECATED_API\", \"0\"))\n\n\n# ----------------------------------------------------------------------\n# Specification of Dependencies\n\n\n# TODO(cython#4518): Need to check to see if e.g. `linetrace` has changed and\n#  possibly re-compile.\ndef maybe_cythonize(extensions, *args, **kwargs):\n    \"\"\"\n    Render tempita templates before calling cythonize. This is skipped for\n\n    * clean\n    * sdist\n    \"\"\"\n    if \"clean\" in sys.argv or \"sdist\" in sys.argv:\n        # See https://github.com/cython/cython/issues/1495\n        return extensions\n\n    elif not _CYTHON_INSTALLED:\n        # GH#28836 raise a helfpul error message\n        if _CYTHON_VERSION:\n            raise RuntimeError(\n                f\"Cannot cythonize with old Cython version ({_CYTHON_VERSION} \"\n                f\"installed, needs {min_cython_ver})\"\n            )\n        raise RuntimeError(\"Cannot cythonize without Cython installed.\")\n\n    # reuse any parallel arguments provided for compilation to cythonize\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--parallel\", \"-j\", type=int, default=1)\n    parsed, _ = parser.parse_known_args()\n\n    kwargs[\"nthreads\"] = parsed.parallel\n    build_ext.render_templates(_pxifiles)\n    if debugging_symbols_requested:\n        kwargs[\"gdb_debug\"] = True\n\n    return cythonize(extensions, *args, **kwargs)\n\n\ndef srcpath(name=None, suffix=\".pyx\", subdir=\"src\"):\n    return pjoin(\"pandas\", subdir, name + suffix)\n\n\nlib_depends = [\"pandas/_libs/include/pandas/parse_helper.h\"]\n\ntseries_depends = [\n    \"pandas/_libs/include/pandas/datetime/pd_datetime.h\",\n]\n\next_data = {\n    \"_libs.algos\": {\n        \"pyxfile\": \"_libs/algos\",\n        \"depends\": _pxi_dep[\"algos\"],\n    },\n    \"_libs.arrays\": {\"pyxfile\": \"_libs/arrays\"},\n    \"_libs.groupby\": {\"pyxfile\": \"_libs/groupby\"},\n    \"_libs.hashing\": {\"pyxfile\": \"_libs/hashing\", \"depends\": []},\n    \"_libs.hashtable\": {\n        \"pyxfile\": \"_libs/hashtable\",\n        \"depends\": (\n            [\n                \"pandas/_libs/include/pandas/vendored/klib/khash_python.h\",\n                \"pandas/_libs/include/pandas/vendored/klib/khash.h\",\n            ]\n            + _pxi_dep[\"hashtable\"]\n        ),\n    },\n    \"_libs.index\": {\n        \"pyxfile\": \"_libs/index\",\n        \"depends\": _pxi_dep[\"index\"],\n    },\n    \"_libs.indexing\": {\"pyxfile\": \"_libs/indexing\"},\n    \"_libs.internals\": {\"pyxfile\": \"_libs/internals\"},\n    \"_libs.interval\": {\n        \"pyxfile\": \"_libs/interval\",\n        \"depends\": _pxi_dep[\"interval\"],\n    },\n    \"_libs.join\": {\"pyxfile\": \"_libs/join\"},\n    \"_libs.lib\": {\n        \"pyxfile\": \"_libs/lib\",\n        \"depends\": lib_depends + tseries_depends,\n    },\n    \"_libs.missing\": {\"pyxfile\": \"_libs/missing\", \"depends\": tseries_depends},\n    \"_libs.parsers\": {\n        \"pyxfile\": \"_libs/parsers\",\n        \"depends\": [\n            \"pandas/_libs/src/parser/tokenizer.h\",\n            \"pandas/_libs/src/parser/io.h\",\n            \"pandas/_libs/src/pd_parser.h\",\n        ],\n    },\n    \"_libs.ops\": {\"pyxfile\": \"_libs/ops\"},\n    \"_libs.ops_dispatch\": {\"pyxfile\": \"_libs/ops_dispatch\"},\n    \"_libs.properties\": {\"pyxfile\": \"_libs/properties\"},\n    \"_libs.reshape\": {\"pyxfile\": \"_libs/reshape\", \"depends\": []},\n    \"_libs.sparse\": {\"pyxfile\": \"_libs/sparse\", \"depends\": _pxi_dep[\"sparse\"]},\n    \"_libs.tslib\": {\n        \"pyxfile\": \"_libs/tslib\",\n        \"depends\": tseries_depends,\n    },\n    \"_libs.tslibs.base\": {\"pyxfile\": \"_libs/tslibs/base\"},\n    \"_libs.tslibs.ccalendar\": {\"pyxfile\": \"_libs/tslibs/ccalendar\"},\n    \"_libs.tslibs.dtypes\": {\"pyxfile\": \"_libs/tslibs/dtypes\"},\n    \"_libs.tslibs.conversion\": {\n        \"pyxfile\": \"_libs/tslibs/conversion\",\n        \"depends\": tseries_depends,\n    },\n    \"_libs.tslibs.fields\": {\n        \"pyxfile\": \"_libs/tslibs/fields\",\n        \"depends\": tseries_depends,\n    },\n    \"_libs.tslibs.nattype\": {\"pyxfile\": \"_libs/tslibs/nattype\"},\n    \"_libs.tslibs.np_datetime\": {\n        \"pyxfile\": \"_libs/tslibs/np_datetime\",\n        \"depends\": tseries_depends,\n    },\n    \"_libs.tslibs.offsets\": {\n        \"pyxfile\": \"_libs/tslibs/offsets\",\n        \"depends\": tseries_depends,\n    },\n    \"_libs.tslibs.parsing\": {\n        \"pyxfile\": \"_libs/tslibs/parsing\",\n        \"sources\": [\"pandas/_libs/src/parser/tokenizer.c\"],\n    },\n    \"_libs.tslibs.period\": {\n        \"pyxfile\": \"_libs/tslibs/period\",\n        \"depends\": tseries_depends,\n    },\n    \"_libs.tslibs.strptime\": {\n        \"pyxfile\": \"_libs/tslibs/strptime\",\n        \"depends\": tseries_depends,\n    },\n    \"_libs.tslibs.timedeltas\": {\n        \"pyxfile\": \"_libs/tslibs/timedeltas\",\n        \"depends\": tseries_depends,\n    },\n    \"_libs.tslibs.timestamps\": {\n        \"pyxfile\": \"_libs/tslibs/timestamps\",\n        \"depends\": tseries_depends,\n    },\n    \"_libs.tslibs.timezones\": {\"pyxfile\": \"_libs/tslibs/timezones\"},\n    \"_libs.tslibs.tzconversion\": {\n        \"pyxfile\": \"_libs/tslibs/tzconversion\",\n        \"depends\": tseries_depends,\n    },\n    \"_libs.tslibs.vectorized\": {\n        \"pyxfile\": \"_libs/tslibs/vectorized\",\n        \"depends\": tseries_depends,\n    },\n    \"_libs.testing\": {\"pyxfile\": \"_libs/testing\"},\n    \"_libs.window.aggregations\": {\n        \"pyxfile\": \"_libs/window/aggregations\",\n        \"language\": \"c++\",\n        \"suffix\": \".cpp\",\n        \"depends\": [\"pandas/_libs/include/pandas/skiplist.h\"],\n    },\n    \"_libs.window.indexers\": {\"pyxfile\": \"_libs/window/indexers\"},\n    \"_libs.writers\": {\"pyxfile\": \"_libs/writers\"},\n    \"_libs.sas\": {\"pyxfile\": \"_libs/sas\"},\n    \"_libs.byteswap\": {\"pyxfile\": \"_libs/byteswap\"},\n}\n\nextensions = []\n\nfor name, data in ext_data.items():\n    source_suffix = suffix if suffix == \".pyx\" else data.get(\"suffix\", \".c\")\n\n    sources = [srcpath(data[\"pyxfile\"], suffix=source_suffix, subdir=\"\")]\n\n    sources.extend(data.get(\"sources\", []))\n\n    include = [\"pandas/_libs/include\", numpy.get_include()]\n\n    undef_macros = []\n\n    if (\n        sys.platform == \"zos\"\n        and data.get(\"language\") == \"c++\"\n        and os.path.basename(os.environ.get(\"CXX\", \"/bin/xlc++\")) in (\"xlc\", \"xlc++\")\n    ):\n        data.get(\"macros\", macros).append((\"__s390__\", \"1\"))\n        extra_compile_args.append(\"-qlanglvl=extended0x:nolibext\")\n        undef_macros.append(\"_POSIX_THREADS\")\n\n    obj = Extension(\n        f\"pandas.{name}\",\n        sources=sources,\n        depends=data.get(\"depends\", []),\n        include_dirs=include,\n        language=data.get(\"language\", \"c\"),\n        define_macros=data.get(\"macros\", macros),\n        extra_compile_args=extra_compile_args,\n        extra_link_args=extra_link_args,\n        undef_macros=undef_macros,\n    )\n\n    extensions.append(obj)\n\n# ----------------------------------------------------------------------\n# ujson\n\nif suffix == \".pyx\":\n    # undo dumb setuptools bug clobbering .pyx sources back to .c\n    for ext in extensions:\n        if ext.sources[0].endswith((\".c\", \".cpp\")):\n            root, _ = os.path.splitext(ext.sources[0])\n            ext.sources[0] = root + suffix\n\nujson_ext = Extension(\n    \"pandas._libs.json\",\n    depends=[\n        \"pandas/_libs/include/pandas/vendored/ujson/lib/ultrajson.h\",\n        \"pandas/_libs/include/pandas/datetime/pd_datetime.h\",\n    ],\n    sources=(\n        [\n            \"pandas/_libs/src/vendored/ujson/python/ujson.c\",\n            \"pandas/_libs/src/vendored/ujson/python/objToJSON.c\",\n            \"pandas/_libs/src/vendored/ujson/python/JSONtoObj.c\",\n            \"pandas/_libs/src/vendored/ujson/lib/ultrajsonenc.c\",\n            \"pandas/_libs/src/vendored/ujson/lib/ultrajsondec.c\",\n        ]\n    ),\n    include_dirs=[\n        \"pandas/_libs/include\",\n        numpy.get_include(),\n    ],\n    extra_compile_args=(extra_compile_args),\n    extra_link_args=extra_link_args,\n    define_macros=macros,\n)\n\n\nextensions.append(ujson_ext)\n\n# ----------------------------------------------------------------------\n\n# ----------------------------------------------------------------------\n# pd_datetime\npd_dt_ext = Extension(\n    \"pandas._libs.pandas_datetime\",\n    depends=[\"pandas/_libs/tslibs/datetime/pd_datetime.h\"],\n    sources=(\n        [\n            \"pandas/_libs/src/vendored/numpy/datetime/np_datetime.c\",\n            \"pandas/_libs/src/vendored/numpy/datetime/np_datetime_strings.c\",\n            \"pandas/_libs/src/datetime/date_conversions.c\",\n            \"pandas/_libs/src/datetime/pd_datetime.c\",\n        ]\n    ),\n    include_dirs=[\n        \"pandas/_libs/include\",\n        numpy.get_include(),\n    ],\n    extra_compile_args=(extra_compile_args),\n    extra_link_args=extra_link_args,\n    define_macros=macros,\n)\n\n\nextensions.append(pd_dt_ext)\n\n# ----------------------------------------------------------------------\n\n# ----------------------------------------------------------------------\n# pd_datetime\npd_parser_ext = Extension(\n    \"pandas._libs.pandas_parser\",\n    depends=[\"pandas/_libs/include/pandas/parser/pd_parser.h\"],\n    sources=(\n        [\n            \"pandas/_libs/src/parser/tokenizer.c\",\n            \"pandas/_libs/src/parser/io.c\",\n            \"pandas/_libs/src/parser/pd_parser.c\",\n        ]\n    ),\n    include_dirs=[\n        \"pandas/_libs/include\",\n    ],\n    extra_compile_args=(extra_compile_args),\n    extra_link_args=extra_link_args,\n    define_macros=macros,\n)\n\n\nextensions.append(pd_parser_ext)\n\n\n# ----------------------------------------------------------------------\n\n\nif __name__ == \"__main__\":\n    # Freeze to support parallel compilation when using spawn instead of fork\n    multiprocessing.freeze_support()\n    setup(\n        version=versioneer.get_version(),\n        ext_modules=maybe_cythonize(extensions, compiler_directives=directives),\n        cmdclass=cmdclass,\n    )\n"
    },
    {
        "repository": "scikit-learn/scikit-learn",
        "file_name": "conftest.py",
        "content": "# Even if empty this file is useful so that when running from the root folder\n# ./sklearn is added to sys.path by pytest. See\n# https://docs.pytest.org/en/latest/explanation/pythonpath.html for more\n# details. For example, this allows to build extensions in place and run pytest\n# doc/modules/clustering.rst and use sklearn from the local folder rather than\n# the one from site-packages.\n"
    },
    {
        "repository": "scikit-learn/scikit-learn",
        "file_name": "setup.py",
        "content": "#! /usr/bin/env python\n#\n# Copyright (C) 2007-2009 Cournapeau David <cournape@gmail.com>\n#               2010 Fabian Pedregosa <fabian.pedregosa@inria.fr>\n# License: 3-clause BSD\n\nimport importlib\nimport os\nimport platform\nimport shutil\nimport sys\nimport traceback\nfrom os.path import join\n\nfrom setuptools import Command, Extension, setup\nfrom setuptools.command.build_ext import build_ext\n\ntry:\n    import builtins\nexcept ImportError:\n    # Python 2 compat: just to be able to declare that Python >=3.8 is needed.\n    import __builtin__ as builtins\n\n# This is a bit (!) hackish: we are setting a global variable so that the main\n# sklearn __init__ can detect if it is being loaded by the setup routine, to\n# avoid attempting to load components that aren't built yet.\n# TODO: can this be simplified or removed since the switch to setuptools\n# away from numpy.distutils?\nbuiltins.__SKLEARN_SETUP__ = True\n\n\nDISTNAME = \"scikit-learn\"\nDESCRIPTION = \"A set of python modules for machine learning and data mining\"\nwith open(\"README.rst\") as f:\n    LONG_DESCRIPTION = f.read()\nMAINTAINER = \"Andreas Mueller\"\nMAINTAINER_EMAIL = \"amueller@ais.uni-bonn.de\"\nURL = \"https://scikit-learn.org\"\nDOWNLOAD_URL = \"https://pypi.org/project/scikit-learn/#files\"\nLICENSE = \"new BSD\"\nPROJECT_URLS = {\n    \"Bug Tracker\": \"https://github.com/scikit-learn/scikit-learn/issues\",\n    \"Documentation\": \"https://scikit-learn.org/stable/documentation.html\",\n    \"Source Code\": \"https://github.com/scikit-learn/scikit-learn\",\n}\n\n# We can actually import a restricted version of sklearn that\n# does not need the compiled code\nimport sklearn  # noqa\nimport sklearn._min_dependencies as min_deps  # noqa\nfrom sklearn._build_utils import _check_cython_version  # noqa\nfrom sklearn.externals._packaging.version import parse as parse_version  # noqa\n\n\nVERSION = sklearn.__version__\n\n# Custom clean command to remove build artifacts\n\n\nclass CleanCommand(Command):\n    description = \"Remove build artifacts from the source tree\"\n\n    user_options = []\n\n    def initialize_options(self):\n        pass\n\n    def finalize_options(self):\n        pass\n\n    def run(self):\n        # Remove c files if we are not within a sdist package\n        cwd = os.path.abspath(os.path.dirname(__file__))\n        remove_c_files = not os.path.exists(os.path.join(cwd, \"PKG-INFO\"))\n        if remove_c_files:\n            print(\"Will remove generated .c files\")\n        if os.path.exists(\"build\"):\n            shutil.rmtree(\"build\")\n        for dirpath, dirnames, filenames in os.walk(\"sklearn\"):\n            for filename in filenames:\n                root, extension = os.path.splitext(filename)\n\n                if extension in [\".so\", \".pyd\", \".dll\", \".pyc\"]:\n                    os.unlink(os.path.join(dirpath, filename))\n\n                if remove_c_files and extension in [\".c\", \".cpp\"]:\n                    pyx_file = str.replace(filename, extension, \".pyx\")\n                    if os.path.exists(os.path.join(dirpath, pyx_file)):\n                        os.unlink(os.path.join(dirpath, filename))\n\n                if remove_c_files and extension == \".tp\":\n                    if os.path.exists(os.path.join(dirpath, root)):\n                        os.unlink(os.path.join(dirpath, root))\n\n            for dirname in dirnames:\n                if dirname == \"__pycache__\":\n                    shutil.rmtree(os.path.join(dirpath, dirname))\n\n\n# Custom build_ext command to set OpenMP compile flags depending on os and\n# compiler. Also makes it possible to set the parallelism level via\n# and environment variable (useful for the wheel building CI).\n# build_ext has to be imported after setuptools\n\n\nclass build_ext_subclass(build_ext):\n    def finalize_options(self):\n        build_ext.finalize_options(self)\n        if self.parallel is None:\n            # Do not override self.parallel if already defined by\n            # command-line flag (--parallel or -j)\n\n            parallel = os.environ.get(\"SKLEARN_BUILD_PARALLEL\")\n            if parallel:\n                self.parallel = int(parallel)\n        if self.parallel:\n            print(\"setting parallel=%d \" % self.parallel)\n\n    def build_extensions(self):\n        from sklearn._build_utils.openmp_helpers import get_openmp_flag\n\n        # Always use NumPy 1.7 C API for all compiled extensions.\n        # See: https://numpy.org/doc/stable/reference/c-api/deprecations.html\n        DEFINE_MACRO_NUMPY_C_API = (\n            \"NPY_NO_DEPRECATED_API\",\n            \"NPY_1_7_API_VERSION\",\n        )\n        for ext in self.extensions:\n            ext.define_macros.append(DEFINE_MACRO_NUMPY_C_API)\n\n        if sklearn._OPENMP_SUPPORTED:\n            openmp_flag = get_openmp_flag()\n\n            for e in self.extensions:\n                e.extra_compile_args += openmp_flag\n                e.extra_link_args += openmp_flag\n\n        build_ext.build_extensions(self)\n\n    def run(self):\n        # Specifying `build_clib` allows running `python setup.py develop`\n        # fully from a fresh clone.\n        self.run_command(\"build_clib\")\n        build_ext.run(self)\n\n\ncmdclass = {\n    \"clean\": CleanCommand,\n    \"build_ext\": build_ext_subclass,\n}\n\n\ndef check_package_status(package, min_version):\n    \"\"\"\n    Returns a dictionary containing a boolean specifying whether given package\n    is up-to-date, along with the version string (empty string if\n    not installed).\n    \"\"\"\n    package_status = {}\n    try:\n        module = importlib.import_module(package)\n        package_version = module.__version__\n        package_status[\"up_to_date\"] = parse_version(package_version) >= parse_version(\n            min_version\n        )\n        package_status[\"version\"] = package_version\n    except ImportError:\n        traceback.print_exc()\n        package_status[\"up_to_date\"] = False\n        package_status[\"version\"] = \"\"\n\n    req_str = \"scikit-learn requires {} >= {}.\\n\".format(package, min_version)\n\n    instructions = (\n        \"Installation instructions are available on the \"\n        \"scikit-learn website: \"\n        \"https://scikit-learn.org/stable/install.html\\n\"\n    )\n\n    if package_status[\"up_to_date\"] is False:\n        if package_status[\"version\"]:\n            raise ImportError(\n                \"Your installation of {} {} is out-of-date.\\n{}{}\".format(\n                    package, package_status[\"version\"], req_str, instructions\n                )\n            )\n        else:\n            raise ImportError(\n                \"{} is not installed.\\n{}{}\".format(package, req_str, instructions)\n            )\n\n\nextension_config = {\n    \"__check_build\": [\n        {\"sources\": [\"_check_build.pyx\"]},\n    ],\n    \"\": [\n        {\"sources\": [\"_isotonic.pyx\"]},\n    ],\n    \"_loss\": [\n        {\"sources\": [\"_loss.pyx.tp\"]},\n    ],\n    \"cluster\": [\n        {\"sources\": [\"_dbscan_inner.pyx\"], \"language\": \"c++\", \"include_np\": True},\n        {\"sources\": [\"_hierarchical_fast.pyx\"], \"language\": \"c++\", \"include_np\": True},\n        {\"sources\": [\"_k_means_common.pyx\"], \"include_np\": True},\n        {\"sources\": [\"_k_means_lloyd.pyx\"], \"include_np\": True},\n        {\"sources\": [\"_k_means_elkan.pyx\"], \"include_np\": True},\n        {\"sources\": [\"_k_means_minibatch.pyx\"], \"include_np\": True},\n    ],\n    \"cluster._hdbscan\": [\n        {\"sources\": [\"_linkage.pyx\"], \"include_np\": True},\n        {\"sources\": [\"_reachability.pyx\"], \"include_np\": True},\n        {\"sources\": [\"_tree.pyx\"], \"include_np\": True},\n    ],\n    \"datasets\": [\n        {\n            \"sources\": [\"_svmlight_format_fast.pyx\"],\n            \"include_np\": True,\n            \"compile_for_pypy\": False,\n        }\n    ],\n    \"decomposition\": [\n        {\"sources\": [\"_online_lda_fast.pyx\"], \"include_np\": True},\n        {\"sources\": [\"_cdnmf_fast.pyx\"], \"include_np\": True},\n    ],\n    \"ensemble\": [\n        {\"sources\": [\"_gradient_boosting.pyx\"], \"include_np\": True},\n    ],\n    \"ensemble._hist_gradient_boosting\": [\n        {\"sources\": [\"_gradient_boosting.pyx\"], \"include_np\": True},\n        {\"sources\": [\"histogram.pyx\"], \"include_np\": True},\n        {\"sources\": [\"splitting.pyx\"], \"include_np\": True},\n        {\"sources\": [\"_binning.pyx\"], \"include_np\": True},\n        {\"sources\": [\"_predictor.pyx\"], \"include_np\": True},\n        {\"sources\": [\"_bitset.pyx\"], \"include_np\": True},\n        {\"sources\": [\"common.pyx\"], \"include_np\": True},\n        {\"sources\": [\"utils.pyx\"], \"include_np\": True},\n    ],\n    \"feature_extraction\": [\n        {\"sources\": [\"_hashing_fast.pyx\"], \"language\": \"c++\", \"include_np\": True},\n    ],\n    \"linear_model\": [\n        {\"sources\": [\"_cd_fast.pyx\"], \"include_np\": True},\n        {\"sources\": [\"_sgd_fast.pyx.tp\"], \"include_np\": True},\n        {\"sources\": [\"_sag_fast.pyx.tp\"], \"include_np\": True},\n    ],\n    \"manifold\": [\n        {\"sources\": [\"_utils.pyx\"], \"include_np\": True},\n        {\"sources\": [\"_barnes_hut_tsne.pyx\"], \"include_np\": True},\n    ],\n    \"metrics\": [\n        {\"sources\": [\"_pairwise_fast.pyx\"], \"include_np\": True},\n        {\n            \"sources\": [\"_dist_metrics.pyx.tp\", \"_dist_metrics.pxd.tp\"],\n            \"include_np\": True,\n        },\n    ],\n    \"metrics.cluster\": [\n        {\"sources\": [\"_expected_mutual_info_fast.pyx\"], \"include_np\": True},\n    ],\n    \"metrics._pairwise_distances_reduction\": [\n        {\n            \"sources\": [\"_datasets_pair.pyx.tp\", \"_datasets_pair.pxd.tp\"],\n            \"language\": \"c++\",\n            \"include_np\": True,\n            \"extra_compile_args\": [\"-std=c++11\"],\n        },\n        {\n            \"sources\": [\"_middle_term_computer.pyx.tp\", \"_middle_term_computer.pxd.tp\"],\n            \"language\": \"c++\",\n            \"extra_compile_args\": [\"-std=c++11\"],\n        },\n        {\n            \"sources\": [\"_base.pyx.tp\", \"_base.pxd.tp\"],\n            \"language\": \"c++\",\n            \"include_np\": True,\n            \"extra_compile_args\": [\"-std=c++11\"],\n        },\n        {\n            \"sources\": [\"_argkmin.pyx.tp\", \"_argkmin.pxd.tp\"],\n            \"language\": \"c++\",\n            \"include_np\": True,\n            \"extra_compile_args\": [\"-std=c++11\"],\n        },\n        {\n            \"sources\": [\"_argkmin_classmode.pyx.tp\"],\n            \"language\": \"c++\",\n            \"include_np\": True,\n            \"extra_compile_args\": [\"-std=c++11\"],\n        },\n        {\n            \"sources\": [\"_radius_neighbors.pyx.tp\", \"_radius_neighbors.pxd.tp\"],\n            \"language\": \"c++\",\n            \"include_np\": True,\n            \"extra_compile_args\": [\"-std=c++11\"],\n        },\n        {\n            \"sources\": [\"_radius_neighbors_classmode.pyx.tp\"],\n            \"language\": \"c++\",\n            \"include_np\": True,\n            \"extra_compile_args\": [\"-std=c++11\"],\n        },\n    ],\n    \"preprocessing\": [\n        {\"sources\": [\"_csr_polynomial_expansion.pyx\"]},\n        {\n            \"sources\": [\"_target_encoder_fast.pyx\"],\n            \"include_np\": True,\n            \"language\": \"c++\",\n            \"extra_compile_args\": [\"-std=c++11\"],\n        },\n    ],\n    \"neighbors\": [\n        {\"sources\": [\"_binary_tree.pxi.tp\"], \"include_np\": True},\n        {\"sources\": [\"_ball_tree.pyx.tp\"], \"include_np\": True},\n        {\"sources\": [\"_kd_tree.pyx.tp\"], \"include_np\": True},\n        {\"sources\": [\"_partition_nodes.pyx\"], \"language\": \"c++\", \"include_np\": True},\n        {\"sources\": [\"_quad_tree.pyx\"], \"include_np\": True},\n    ],\n    \"svm\": [\n        {\n            \"sources\": [\"_newrand.pyx\"],\n            \"include_np\": True,\n            \"include_dirs\": [join(\"src\", \"newrand\")],\n            \"language\": \"c++\",\n            # Use C++11 random number generator fix\n            \"extra_compile_args\": [\"-std=c++11\"],\n        },\n        {\n            \"sources\": [\"_libsvm.pyx\"],\n            \"depends\": [\n                join(\"src\", \"libsvm\", \"libsvm_helper.c\"),\n                join(\"src\", \"libsvm\", \"libsvm_template.cpp\"),\n                join(\"src\", \"libsvm\", \"svm.cpp\"),\n                join(\"src\", \"libsvm\", \"svm.h\"),\n                join(\"src\", \"newrand\", \"newrand.h\"),\n            ],\n            \"include_dirs\": [\n                join(\"src\", \"libsvm\"),\n                join(\"src\", \"newrand\"),\n            ],\n            \"libraries\": [\"libsvm-skl\"],\n            \"extra_link_args\": [\"-lstdc++\"],\n            \"include_np\": True,\n        },\n        {\n            \"sources\": [\"_liblinear.pyx\"],\n            \"libraries\": [\"liblinear-skl\"],\n            \"include_dirs\": [\n                join(\"src\", \"liblinear\"),\n                join(\"src\", \"newrand\"),\n                join(\"..\", \"utils\"),\n            ],\n            \"include_np\": True,\n            \"depends\": [\n                join(\"src\", \"liblinear\", \"tron.h\"),\n                join(\"src\", \"liblinear\", \"linear.h\"),\n                join(\"src\", \"liblinear\", \"liblinear_helper.c\"),\n                join(\"src\", \"newrand\", \"newrand.h\"),\n            ],\n            \"extra_link_args\": [\"-lstdc++\"],\n        },\n        {\n            \"sources\": [\"_libsvm_sparse.pyx\"],\n            \"libraries\": [\"libsvm-skl\"],\n            \"include_dirs\": [\n                join(\"src\", \"libsvm\"),\n                join(\"src\", \"newrand\"),\n            ],\n            \"include_np\": True,\n            \"depends\": [\n                join(\"src\", \"libsvm\", \"svm.h\"),\n                join(\"src\", \"newrand\", \"newrand.h\"),\n                join(\"src\", \"libsvm\", \"libsvm_sparse_helper.c\"),\n            ],\n            \"extra_link_args\": [\"-lstdc++\"],\n        },\n    ],\n    \"tree\": [\n        {\n            \"sources\": [\"_tree.pyx\"],\n            \"language\": \"c++\",\n            \"include_np\": True,\n            \"optimization_level\": \"O3\",\n        },\n        {\"sources\": [\"_splitter.pyx\"], \"include_np\": True, \"optimization_level\": \"O3\"},\n        {\"sources\": [\"_criterion.pyx\"], \"include_np\": True, \"optimization_level\": \"O3\"},\n        {\"sources\": [\"_utils.pyx\"], \"include_np\": True, \"optimization_level\": \"O3\"},\n    ],\n    \"utils\": [\n        {\"sources\": [\"sparsefuncs_fast.pyx\"], \"include_np\": True},\n        {\"sources\": [\"_cython_blas.pyx\"]},\n        {\"sources\": [\"arrayfuncs.pyx\"]},\n        {\n            \"sources\": [\"murmurhash.pyx\", join(\"src\", \"MurmurHash3.cpp\")],\n            \"include_dirs\": [\"src\"],\n            \"include_np\": True,\n        },\n        {\"sources\": [\"_fast_dict.pyx\"], \"language\": \"c++\"},\n        {\"sources\": [\"_openmp_helpers.pyx\"]},\n        {\"sources\": [\"_seq_dataset.pyx.tp\", \"_seq_dataset.pxd.tp\"], \"include_np\": True},\n        {\n            \"sources\": [\"_weight_vector.pyx.tp\", \"_weight_vector.pxd.tp\"],\n            \"include_np\": True,\n        },\n        {\"sources\": [\"_random.pyx\"], \"include_np\": True},\n        {\"sources\": [\"_typedefs.pyx\"]},\n        {\"sources\": [\"_heap.pyx\"]},\n        {\"sources\": [\"_sorting.pyx\"]},\n        {\"sources\": [\"_vector_sentinel.pyx\"], \"language\": \"c++\", \"include_np\": True},\n        {\"sources\": [\"_isfinite.pyx\"]},\n    ],\n}\n\n# Paths in `libraries` must be relative to the root directory because `libraries` is\n# passed directly to `setup`\nlibraries = [\n    (\n        \"libsvm-skl\",\n        {\n            \"sources\": [\n                join(\"sklearn\", \"svm\", \"src\", \"libsvm\", \"libsvm_template.cpp\"),\n            ],\n            \"depends\": [\n                join(\"sklearn\", \"svm\", \"src\", \"libsvm\", \"svm.cpp\"),\n                join(\"sklearn\", \"svm\", \"src\", \"libsvm\", \"svm.h\"),\n                join(\"sklearn\", \"svm\", \"src\", \"newrand\", \"newrand.h\"),\n            ],\n            # Use C++11 to use the random number generator fix\n            \"extra_compiler_args\": [\"-std=c++11\"],\n            \"extra_link_args\": [\"-lstdc++\"],\n        },\n    ),\n    (\n        \"liblinear-skl\",\n        {\n            \"sources\": [\n                join(\"sklearn\", \"svm\", \"src\", \"liblinear\", \"linear.cpp\"),\n                join(\"sklearn\", \"svm\", \"src\", \"liblinear\", \"tron.cpp\"),\n            ],\n            \"depends\": [\n                join(\"sklearn\", \"svm\", \"src\", \"liblinear\", \"linear.h\"),\n                join(\"sklearn\", \"svm\", \"src\", \"liblinear\", \"tron.h\"),\n                join(\"sklearn\", \"svm\", \"src\", \"newrand\", \"newrand.h\"),\n            ],\n            # Use C++11 to use the random number generator fix\n            \"extra_compiler_args\": [\"-std=c++11\"],\n            \"extra_link_args\": [\"-lstdc++\"],\n        },\n    ),\n]\n\n\ndef configure_extension_modules():\n    # Skip cythonization as we do not want to include the generated\n    # C/C++ files in the release tarballs as they are not necessarily\n    # forward compatible with future versions of Python for instance.\n    if \"sdist\" in sys.argv or \"--help\" in sys.argv:\n        return []\n\n    import numpy\n\n    from sklearn._build_utils import cythonize_extensions, gen_from_templates\n\n    is_pypy = platform.python_implementation() == \"PyPy\"\n    np_include = numpy.get_include()\n    default_optimization_level = \"O2\"\n\n    if os.name == \"posix\":\n        default_libraries = [\"m\"]\n    else:\n        default_libraries = []\n\n    default_extra_compile_args = []\n    build_with_debug_symbols = (\n        os.environ.get(\"SKLEARN_BUILD_ENABLE_DEBUG_SYMBOLS\", \"0\") != \"0\"\n    )\n    if os.name == \"posix\":\n        if build_with_debug_symbols:\n            default_extra_compile_args.append(\"-g\")\n        else:\n            # Setting -g0 will strip symbols, reducing the binary size of extensions\n            default_extra_compile_args.append(\"-g0\")\n\n    cython_exts = []\n    for submodule, extensions in extension_config.items():\n        submodule_parts = submodule.split(\".\")\n        parent_dir = join(\"sklearn\", *submodule_parts)\n        for extension in extensions:\n            if is_pypy and not extension.get(\"compile_for_pypy\", True):\n                continue\n\n            # Generate files with Tempita\n            tempita_sources = []\n            sources = []\n            for source in extension[\"sources\"]:\n                source = join(parent_dir, source)\n                new_source_path, path_ext = os.path.splitext(source)\n\n                if path_ext != \".tp\":\n                    sources.append(source)\n                    continue\n\n                # `source` is a Tempita file\n                tempita_sources.append(source)\n\n                # Only include source files that are pyx files\n                if os.path.splitext(new_source_path)[-1] == \".pyx\":\n                    sources.append(new_source_path)\n\n            gen_from_templates(tempita_sources)\n\n            # Do not progress if we only have a tempita file which we don't\n            # want to include like the .pxi.tp extension. In such a case\n            # sources would be empty.\n            if not sources:\n                continue\n\n            # By convention, our extensions always use the name of the first source\n            source_name = os.path.splitext(os.path.basename(sources[0]))[0]\n            if submodule:\n                name_parts = [\"sklearn\", submodule, source_name]\n            else:\n                name_parts = [\"sklearn\", source_name]\n            name = \".\".join(name_parts)\n\n            # Make paths start from the root directory\n            include_dirs = [\n                join(parent_dir, include_dir)\n                for include_dir in extension.get(\"include_dirs\", [])\n            ]\n            if extension.get(\"include_np\", False):\n                include_dirs.append(np_include)\n\n            depends = [\n                join(parent_dir, depend) for depend in extension.get(\"depends\", [])\n            ]\n\n            extra_compile_args = (\n                extension.get(\"extra_compile_args\", []) + default_extra_compile_args\n            )\n            optimization_level = extension.get(\n                \"optimization_level\", default_optimization_level\n            )\n            if os.name == \"posix\":\n                extra_compile_args.append(f\"-{optimization_level}\")\n            else:\n                extra_compile_args.append(f\"/{optimization_level}\")\n\n            libraries_ext = extension.get(\"libraries\", []) + default_libraries\n\n            new_ext = Extension(\n                name=name,\n                sources=sources,\n                language=extension.get(\"language\", None),\n                include_dirs=include_dirs,\n                libraries=libraries_ext,\n                depends=depends,\n                extra_link_args=extension.get(\"extra_link_args\", None),\n                extra_compile_args=extra_compile_args,\n            )\n            cython_exts.append(new_ext)\n\n    return cythonize_extensions(cython_exts)\n\n\ndef setup_package():\n    python_requires = \">=3.8\"\n    required_python_version = (3, 8)\n\n    metadata = dict(\n        name=DISTNAME,\n        maintainer=MAINTAINER,\n        maintainer_email=MAINTAINER_EMAIL,\n        description=DESCRIPTION,\n        license=LICENSE,\n        url=URL,\n        download_url=DOWNLOAD_URL,\n        project_urls=PROJECT_URLS,\n        version=VERSION,\n        long_description=LONG_DESCRIPTION,\n        classifiers=[\n            \"Intended Audience :: Science/Research\",\n            \"Intended Audience :: Developers\",\n            \"License :: OSI Approved :: BSD License\",\n            \"Programming Language :: C\",\n            \"Programming Language :: Python\",\n            \"Topic :: Software Development\",\n            \"Topic :: Scientific/Engineering\",\n            \"Development Status :: 5 - Production/Stable\",\n            \"Operating System :: Microsoft :: Windows\",\n            \"Operating System :: POSIX\",\n            \"Operating System :: Unix\",\n            \"Operating System :: MacOS\",\n            \"Programming Language :: Python :: 3\",\n            \"Programming Language :: Python :: 3.8\",\n            \"Programming Language :: Python :: 3.9\",\n            \"Programming Language :: Python :: 3.10\",\n            \"Programming Language :: Python :: 3.11\",\n            \"Programming Language :: Python :: 3.12\",\n            \"Programming Language :: Python :: Implementation :: CPython\",\n            \"Programming Language :: Python :: Implementation :: PyPy\",\n        ],\n        cmdclass=cmdclass,\n        python_requires=python_requires,\n        install_requires=min_deps.tag_to_packages[\"install\"],\n        package_data={\n            \"\": [\"*.csv\", \"*.gz\", \"*.txt\", \"*.pxd\", \"*.rst\", \"*.jpg\", \"*.css\"]\n        },\n        zip_safe=False,  # the package can run out of an .egg file\n        extras_require={\n            key: min_deps.tag_to_packages[key]\n            for key in [\"examples\", \"docs\", \"tests\", \"benchmark\"]\n        },\n    )\n\n    commands = [arg for arg in sys.argv[1:] if not arg.startswith(\"-\")]\n    if not all(\n        command in (\"egg_info\", \"dist_info\", \"clean\", \"check\") for command in commands\n    ):\n        if sys.version_info < required_python_version:\n            required_version = \"%d.%d\" % required_python_version\n            raise RuntimeError(\n                \"Scikit-learn requires Python %s or later. The current\"\n                \" Python version is %s installed in %s.\"\n                % (required_version, platform.python_version(), sys.executable)\n            )\n\n        check_package_status(\"numpy\", min_deps.NUMPY_MIN_VERSION)\n        check_package_status(\"scipy\", min_deps.SCIPY_MIN_VERSION)\n\n        _check_cython_version()\n        metadata[\"ext_modules\"] = configure_extension_modules()\n        metadata[\"libraries\"] = libraries\n    setup(**metadata)\n\n\nif __name__ == \"__main__\":\n    setup_package()\n"
    },
    {
        "repository": "psf/requests-html",
        "file_name": "requests_html.py",
        "content": "import sys\nimport asyncio\nfrom urllib.parse import urlparse, urlunparse, urljoin\nfrom concurrent.futures import ThreadPoolExecutor\nfrom concurrent.futures._base import TimeoutError\nfrom functools import partial\nfrom typing import Set, Union, List, MutableMapping, Optional\n\nimport pyppeteer\nimport requests\nimport http.cookiejar\nfrom pyquery import PyQuery\n\nfrom fake_useragent import UserAgent\nfrom lxml.html.clean import Cleaner\nimport lxml\nfrom lxml import etree\nfrom lxml.html import HtmlElement\nfrom lxml.html import tostring as lxml_html_tostring\nfrom lxml.html.soupparser import fromstring as soup_parse\nfrom parse import search as parse_search\nfrom parse import findall, Result\nfrom w3lib.encoding import html_to_unicode\n\nDEFAULT_ENCODING = 'utf-8'\nDEFAULT_URL = 'https://example.org/'\nDEFAULT_USER_AGENT = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/603.3.8 (KHTML, like Gecko) Version/10.1.2 Safari/603.3.8'\nDEFAULT_NEXT_SYMBOL = ['next', 'more', 'older']\n\ncleaner = Cleaner()\ncleaner.javascript = True\ncleaner.style = True\n\nuseragent = None\n\n# Typing.\n_Find = Union[List['Element'], 'Element']\n_XPath = Union[List[str], List['Element'], str, 'Element']\n_Result = Union[List['Result'], 'Result']\n_HTML = Union[str, bytes]\n_BaseHTML = str\n_UserAgent = str\n_DefaultEncoding = str\n_URL = str\n_RawHTML = bytes\n_Encoding = str\n_LXML = HtmlElement\n_Text = str\n_Search = Result\n_Containing = Union[str, List[str]]\n_Links = Set[str]\n_Attrs = MutableMapping\n_Next = Union['HTML', List[str]]\n_NextSymbol = List[str]\n\n# Sanity checking.\ntry:\n    assert sys.version_info.major == 3\n    assert sys.version_info.minor > 5\nexcept AssertionError:\n    raise RuntimeError('Requests-HTML requires Python 3.6+!')\n\n\nclass MaxRetries(Exception):\n\n    def __init__(self, message):\n        self.message = message\n\n\nclass BaseParser:\n    \"\"\"A basic HTML/Element Parser, for Humans.\n\n    :param element: The element from which to base the parsing upon.\n    :param default_encoding: Which encoding to default to.\n    :param html: HTML from which to base the parsing upon (optional).\n    :param url: The URL from which the HTML originated, used for ``absolute_links``.\n\n    \"\"\"\n\n    def __init__(self, *, element, default_encoding: _DefaultEncoding = None, html: _HTML = None, url: _URL) -> None:\n        self.element = element\n        self.url = url\n        self.skip_anchors = True\n        self.default_encoding = default_encoding\n        self._encoding = None\n        self._html = html.encode(DEFAULT_ENCODING) if isinstance(html, str) else html\n        self._lxml = None\n        self._pq = None\n\n    @property\n    def raw_html(self) -> _RawHTML:\n        \"\"\"Bytes representation of the HTML content.\n        (`learn more <http://www.diveintopython3.net/strings.html>`_).\n        \"\"\"\n        if self._html:\n            return self._html\n        else:\n            return etree.tostring(self.element, encoding='unicode').strip().encode(self.encoding)\n\n    @property\n    def html(self) -> _BaseHTML:\n        \"\"\"Unicode representation of the HTML content\n        (`learn more <http://www.diveintopython3.net/strings.html>`_).\n        \"\"\"\n        if self._html:\n            return self.raw_html.decode(self.encoding, errors='replace')\n        else:\n            return etree.tostring(self.element, encoding='unicode').strip()\n\n    @html.setter\n    def html(self, html: str) -> None:\n        self._html = html.encode(self.encoding)\n\n    @raw_html.setter\n    def raw_html(self, html: bytes) -> None:\n        \"\"\"Property setter for self.html.\"\"\"\n        self._html = html\n\n    @property\n    def encoding(self) -> _Encoding:\n        \"\"\"The encoding string to be used, extracted from the HTML and\n        :class:`HTMLResponse <HTMLResponse>` headers.\n        \"\"\"\n        if self._encoding:\n            return self._encoding\n\n        # Scan meta tags for charset.\n        if self._html:\n            self._encoding = html_to_unicode(self.default_encoding, self._html)[0]\n            # Fall back to requests' detected encoding if decode fails.\n            try:\n                self.raw_html.decode(self.encoding, errors='replace')\n            except UnicodeDecodeError:\n                self._encoding = self.default_encoding\n\n\n        return self._encoding if self._encoding else self.default_encoding\n\n    @encoding.setter\n    def encoding(self, enc: str) -> None:\n        \"\"\"Property setter for self.encoding.\"\"\"\n        self._encoding = enc\n\n    @property\n    def pq(self) -> PyQuery:\n        \"\"\"`PyQuery <https://pythonhosted.org/pyquery/>`_ representation\n        of the :class:`Element <Element>` or :class:`HTML <HTML>`.\n        \"\"\"\n        if self._pq is None:\n            self._pq = PyQuery(self.lxml)\n\n        return self._pq\n\n    @property\n    def lxml(self) -> HtmlElement:\n        \"\"\"`lxml <http://lxml.de>`_ representation of the\n        :class:`Element <Element>` or :class:`HTML <HTML>`.\n        \"\"\"\n        if self._lxml is None:\n            try:\n                self._lxml = soup_parse(self.html, features='html.parser')\n            except ValueError:\n                self._lxml = lxml.html.fromstring(self.raw_html)\n\n        return self._lxml\n\n    @property\n    def text(self) -> _Text:\n        \"\"\"The text content of the\n        :class:`Element <Element>` or :class:`HTML <HTML>`.\n        \"\"\"\n        return self.pq.text()\n\n    @property\n    def full_text(self) -> _Text:\n        \"\"\"The full text content (including links) of the\n        :class:`Element <Element>` or :class:`HTML <HTML>`.\n        \"\"\"\n        return self.lxml.text_content()\n\n    def find(self, selector: str = \"*\", *, containing: _Containing = None, clean: bool = False, first: bool = False, _encoding: str = None) -> _Find:\n        \"\"\"Given a CSS Selector, returns a list of\n        :class:`Element <Element>` objects or a single one.\n\n        :param selector: CSS Selector to use.\n        :param clean: Whether or not to sanitize the found HTML of ``<script>`` and ``<style>`` tags.\n        :param containing: If specified, only return elements that contain the provided text.\n        :param first: Whether or not to return just the first result.\n        :param _encoding: The encoding format.\n\n        Example CSS Selectors:\n\n        - ``a``\n        - ``a.someClass``\n        - ``a#someID``\n        - ``a[target=_blank]``\n\n        See W3School's `CSS Selectors Reference\n        <https://www.w3schools.com/cssref/css_selectors.asp>`_\n        for more details.\n\n        If ``first`` is ``True``, only returns the first\n        :class:`Element <Element>` found.\n        \"\"\"\n\n        # Convert a single containing into a list.\n        if isinstance(containing, str):\n            containing = [containing]\n\n        encoding = _encoding or self.encoding\n        elements = [\n            Element(element=found, url=self.url, default_encoding=encoding)\n            for found in self.pq(selector)\n        ]\n\n        if containing:\n            elements_copy = elements.copy()\n            elements = []\n\n            for element in elements_copy:\n                if any([c.lower() in element.full_text.lower() for c in containing]):\n                    elements.append(element)\n\n            elements.reverse()\n\n        # Sanitize the found HTML.\n        if clean:\n            elements_copy = elements.copy()\n            elements = []\n\n            for element in elements_copy:\n                element.raw_html = lxml_html_tostring(cleaner.clean_html(element.lxml))\n                elements.append(element)\n\n        return _get_first_or_list(elements, first)\n\n    def xpath(self, selector: str, *, clean: bool = False, first: bool = False, _encoding: str = None) -> _XPath:\n        \"\"\"Given an XPath selector, returns a list of\n        :class:`Element <Element>` objects or a single one.\n\n        :param selector: XPath Selector to use.\n        :param clean: Whether or not to sanitize the found HTML of ``<script>`` and ``<style>`` tags.\n        :param first: Whether or not to return just the first result.\n        :param _encoding: The encoding format.\n\n        If a sub-selector is specified (e.g. ``//a/@href``), a simple\n        list of results is returned.\n\n        See W3School's `XPath Examples\n        <https://www.w3schools.com/xml/xpath_examples.asp>`_\n        for more details.\n\n        If ``first`` is ``True``, only returns the first\n        :class:`Element <Element>` found.\n        \"\"\"\n        selected = self.lxml.xpath(selector)\n\n        elements = [\n            Element(element=selection, url=self.url, default_encoding=_encoding or self.encoding)\n            if not isinstance(selection, etree._ElementUnicodeResult) else str(selection)\n            for selection in selected\n        ]\n\n        # Sanitize the found HTML.\n        if clean:\n            elements_copy = elements.copy()\n            elements = []\n\n            for element in elements_copy:\n                element.raw_html = lxml_html_tostring(cleaner.clean_html(element.lxml))\n                elements.append(element)\n\n        return _get_first_or_list(elements, first)\n\n    def search(self, template: str) -> Result:\n        \"\"\"Search the :class:`Element <Element>` for the given Parse template.\n\n        :param template: The Parse template to use.\n        \"\"\"\n\n        return parse_search(template, self.html)\n\n    def search_all(self, template: str) -> _Result:\n        \"\"\"Search the :class:`Element <Element>` (multiple times) for the given parse\n        template.\n\n        :param template: The Parse template to use.\n        \"\"\"\n        return [r for r in findall(template, self.html)]\n\n    @property\n    def links(self) -> _Links:\n        \"\"\"All found links on page, in as\u2013is form.\"\"\"\n\n        def gen():\n            for link in self.find('a'):\n\n                try:\n                    href = link.attrs['href'].strip()\n                    if href and not (href.startswith('#') and self.skip_anchors) and not href.startswith(('javascript:', 'mailto:')):\n                        yield href\n                except KeyError:\n                    pass\n\n        return set(gen())\n\n    def _make_absolute(self, link):\n        \"\"\"Makes a given link absolute.\"\"\"\n\n        # Parse the link with stdlib.\n        parsed = urlparse(link)._asdict()\n\n        # If link is relative, then join it with base_url.\n        if not parsed['netloc']:\n            return urljoin(self.base_url, link)\n\n        # Link is absolute; if it lacks a scheme, add one from base_url.\n        if not parsed['scheme']:\n            parsed['scheme'] = urlparse(self.base_url).scheme\n\n            # Reconstruct the URL to incorporate the new scheme.\n            parsed = (v for v in parsed.values())\n            return urlunparse(parsed)\n\n        # Link is absolute and complete with scheme; nothing to be done here.\n        return link\n\n\n    @property\n    def absolute_links(self) -> _Links:\n        \"\"\"All found links on page, in absolute form\n        (`learn more <https://www.navegabem.com/absolute-or-relative-links.html>`_).\n        \"\"\"\n\n        def gen():\n            for link in self.links:\n                yield self._make_absolute(link)\n\n        return set(gen())\n\n    @property\n    def base_url(self) -> _URL:\n        \"\"\"The base URL for the page. Supports the ``<base>`` tag\n        (`learn more <https://www.w3schools.com/tags/tag_base.asp>`_).\"\"\"\n\n        # Support for <base> tag.\n        base = self.find('base', first=True)\n        if base:\n            result = base.attrs.get('href', '').strip()\n            if result:\n                return result\n\n        # Parse the url to separate out the path\n        parsed = urlparse(self.url)._asdict()\n\n        # Remove any part of the path after the last '/'\n        parsed['path'] = '/'.join(parsed['path'].split('/')[:-1]) + '/'\n\n        # Reconstruct the url with the modified path\n        parsed = (v for v in parsed.values())\n        url = urlunparse(parsed)\n\n        return url\n\n\nclass Element(BaseParser):\n    \"\"\"An element of HTML.\n\n    :param element: The element from which to base the parsing upon.\n    :param url: The URL from which the HTML originated, used for ``absolute_links``.\n    :param default_encoding: Which encoding to default to.\n    \"\"\"\n\n    __slots__ = [\n        'element', 'url', 'skip_anchors', 'default_encoding', '_encoding',\n        '_html', '_lxml', '_pq', '_attrs', 'session'\n    ]\n\n    def __init__(self, *, element, url: _URL, default_encoding: _DefaultEncoding = None) -> None:\n        super(Element, self).__init__(element=element, url=url, default_encoding=default_encoding)\n        self.element = element\n        self.tag = element.tag\n        self.lineno = element.sourceline\n        self._attrs = None\n\n    def __repr__(self) -> str:\n        attrs = ['{}={}'.format(attr, repr(self.attrs[attr])) for attr in self.attrs]\n        return \"<Element {} {}>\".format(repr(self.element.tag), ' '.join(attrs))\n\n    @property\n    def attrs(self) -> _Attrs:\n        \"\"\"Returns a dictionary of the attributes of the :class:`Element <Element>`\n        (`learn more <https://www.w3schools.com/tags/ref_attributes.asp>`_).\n        \"\"\"\n        if self._attrs is None:\n            self._attrs = {k: v for k, v in self.element.items()}\n\n            # Split class and rel up, as there are usually many of them:\n            for attr in ['class', 'rel']:\n                if attr in self._attrs:\n                    self._attrs[attr] = tuple(self._attrs[attr].split())\n\n        return self._attrs\n\n\nclass HTML(BaseParser):\n    \"\"\"An HTML document, ready for parsing.\n\n    :param url: The URL from which the HTML originated, used for ``absolute_links``.\n    :param html: HTML from which to base the parsing upon (optional).\n    :param default_encoding: Which encoding to default to.\n    \"\"\"\n\n    def __init__(self, *, session: Union['HTMLSession', 'AsyncHTMLSession'] = None, url: str = DEFAULT_URL, html: _HTML, default_encoding: str = DEFAULT_ENCODING, async_: bool = False) -> None:\n\n        # Convert incoming unicode HTML into bytes.\n        if isinstance(html, str):\n            html = html.encode(DEFAULT_ENCODING)\n\n        pq = PyQuery(html)\n        super(HTML, self).__init__(\n            element=pq('html') or pq.wrapAll('<html></html>')('html'),\n            html=html,\n            url=url,\n            default_encoding=default_encoding\n        )\n        self.session = session or async_ and AsyncHTMLSession() or HTMLSession()\n        self.page = None\n        self.next_symbol = DEFAULT_NEXT_SYMBOL\n\n    def __repr__(self) -> str:\n        return f\"<HTML url={self.url!r}>\"\n\n    def next(self, fetch: bool = False, next_symbol: _NextSymbol = None) -> _Next:\n        \"\"\"Attempts to find the next page, if there is one. If ``fetch``\n        is ``True`` (default), returns :class:`HTML <HTML>` object of\n        next page. If ``fetch`` is ``False``, simply returns the next URL.\n\n        \"\"\"\n        if next_symbol is None:\n            next_symbol = DEFAULT_NEXT_SYMBOL\n\n        def get_next():\n            candidates = self.find('a', containing=next_symbol)\n\n            for candidate in candidates:\n                if candidate.attrs.get('href'):\n                    # Support 'next' rel (e.g. reddit).\n                    if 'next' in candidate.attrs.get('rel', []):\n                        return candidate.attrs['href']\n\n                    # Support 'next' in classnames.\n                    for _class in candidate.attrs.get('class', []):\n                        if 'next' in _class:\n                            return candidate.attrs['href']\n\n                    if 'page' in candidate.attrs['href']:\n                        return candidate.attrs['href']\n\n            try:\n                # Resort to the last candidate.\n                return candidates[-1].attrs['href']\n            except IndexError:\n                return None\n\n        __next = get_next()\n        if __next:\n            url = self._make_absolute(__next)\n        else:\n            return None\n\n        if fetch:\n            return self.session.get(url)\n        else:\n            return url\n\n    def __iter__(self):\n\n        next = self\n\n        while True:\n            yield next\n            try:\n                next = next.next(fetch=True, next_symbol=self.next_symbol).html\n            except AttributeError:\n                break\n\n    def __next__(self):\n        return self.next(fetch=True, next_symbol=self.next_symbol).html\n\n    def __aiter__(self):\n        return self\n\n    async def __anext__(self):\n        while True:\n            url = self.next(fetch=False, next_symbol=self.next_symbol)\n            if not url:\n                break\n            response = await self.session.get(url)\n            return response.html\n\n    def add_next_symbol(self, next_symbol):\n        self.next_symbol.append(next_symbol)\n\n    async def _async_render(self, *, url: str, script: str = None, scrolldown, sleep: int, wait: float, reload, content: Optional[str], timeout: Union[float, int], keep_page: bool, cookies: list = [{}]):\n        \"\"\" Handle page creation and js rendering. Internal use for render/arender methods. \"\"\"\n        try:\n            page = await self.browser.newPage()\n\n            # Wait before rendering the page, to prevent timeouts.\n            await asyncio.sleep(wait)\n\n            if cookies:\n                for cookie in cookies:\n                    if cookie:\n                        await page.setCookie(cookie)\n\n            # Load the given page (GET request, obviously.)\n            if reload:\n                await page.goto(url, options={'timeout': int(timeout * 1000)})\n            else:\n                await page.goto(f'data:text/html,{self.html}', options={'timeout': int(timeout * 1000)})\n\n            result = None\n            if script:\n                result = await page.evaluate(script)\n\n            if scrolldown:\n                for _ in range(scrolldown):\n                    await page._keyboard.down('PageDown')\n                    await asyncio.sleep(sleep)\n            else:\n                await asyncio.sleep(sleep)\n\n            if scrolldown:\n                await page._keyboard.up('PageDown')\n\n            # Return the content of the page, JavaScript evaluated.\n            content = await page.content()\n            if not keep_page:\n                await page.close()\n                page = None\n            return content, result, page\n        except TimeoutError:\n            await page.close()\n            page = None\n            return None\n\n    def _convert_cookiejar_to_render(self, session_cookiejar):\n        \"\"\"\n        Convert HTMLSession.cookies:cookiejar[] for browser.newPage().setCookie\n        \"\"\"\n        # |  setCookie(self, *cookies:dict) -> None\n        # |      Set cookies.\n        # |\n        # |      ``cookies`` should be dictionaries which contain these fields:\n        # |\n        # |      * ``name`` (str): **required**\n        # |      * ``value`` (str): **required**\n        # |      * ``url`` (str)\n        # |      * ``domain`` (str)\n        # |      * ``path`` (str)\n        # |      * ``expires`` (number): Unix time in seconds\n        # |      * ``httpOnly`` (bool)\n        # |      * ``secure`` (bool)\n        # |      * ``sameSite`` (str): ``'Strict'`` or ``'Lax'``\n        cookie_render = {}\n        def __convert(cookiejar, key):\n            try:\n                v = eval (\"cookiejar.\"+key)\n                if not v: kv = ''\n                else: kv = {key: v}\n            except:\n                kv = ''\n            return kv\n\n        keys = [\n            'name',\n            'value',\n            'url',\n            'domain',\n            'path',\n            'sameSite',\n            'expires',\n            'httpOnly',\n            'secure',\n        ]\n        for key in keys:\n            cookie_render.update(__convert(session_cookiejar, key))\n        return cookie_render\n\n    def _convert_cookiesjar_to_render(self):\n        \"\"\"\n        Convert HTMLSession.cookies for browser.newPage().setCookie\n        Return a list of dict\n        \"\"\"\n        cookies_render = []\n        if isinstance(self.session.cookies, http.cookiejar.CookieJar):\n            for cookie in self.session.cookies:\n                cookies_render.append(self._convert_cookiejar_to_render(cookie))\n        return cookies_render\n\n    def render(self, retries: int = 8, script: str = None, wait: float = 0.2, scrolldown=False, sleep: int = 0, reload: bool = True, timeout: Union[float, int] = 8.0, keep_page: bool = False, cookies: list = [{}], send_cookies_session: bool = False):\n        \"\"\"Reloads the response in Chromium, and replaces HTML content\n        with an updated version, with JavaScript executed.\n\n        :param retries: The number of times to retry loading the page in Chromium.\n        :param script: JavaScript to execute upon page load (optional).\n        :param wait: The number of seconds to wait before loading the page, preventing timeouts (optional).\n        :param scrolldown: Integer, if provided, of how many times to page down.\n        :param sleep: Integer, if provided, of how many seconds to sleep after initial render.\n        :param reload: If ``False``, content will not be loaded from the browser, but will be provided from memory.\n        :param keep_page: If ``True`` will allow you to interact with the browser page through ``r.html.page``.\n\n        :param send_cookies_session: If ``True`` send ``HTMLSession.cookies`` convert.\n        :param cookies: If not ``empty`` send ``cookies``.\n\n        If ``scrolldown`` is specified, the page will scrolldown the specified\n        number of times, after sleeping the specified amount of time\n        (e.g. ``scrolldown=10, sleep=1``).\n\n        If just ``sleep`` is provided, the rendering will wait *n* seconds, before\n        returning.\n\n        If ``script`` is specified, it will execute the provided JavaScript at\n        runtime. Example:\n\n        .. code-block:: python\n\n            script = \\\"\\\"\\\"\n                () => {\n                    return {\n                        width: document.documentElement.clientWidth,\n                        height: document.documentElement.clientHeight,\n                        deviceScaleFactor: window.devicePixelRatio,\n                    }\n                }\n            \\\"\\\"\\\"\n\n        Returns the return value of the executed  ``script``, if any is provided:\n\n        .. code-block:: python\n\n            >>> r.html.render(script=script)\n            {'width': 800, 'height': 600, 'deviceScaleFactor': 1}\n\n        Warning: the first time you run this method, it will download\n        Chromium into your home directory (``~/.pyppeteer``).\n        \"\"\"\n\n        self.browser = self.session.browser  # Automatically create a event loop and browser\n        content = None\n\n        # Automatically set Reload to False, if example URL is being used.\n        if self.url == DEFAULT_URL:\n            reload = False\n\n        if send_cookies_session:\n           cookies = self._convert_cookiesjar_to_render()\n\n        for i in range(retries):\n            if not content:\n                try:\n\n                    content, result, page = self.session.loop.run_until_complete(self._async_render(url=self.url, script=script, sleep=sleep, wait=wait, content=self.html, reload=reload, scrolldown=scrolldown, timeout=timeout, keep_page=keep_page, cookies=cookies))\n                except TypeError:\n                    pass\n            else:\n                break\n\n        if not content:\n            raise MaxRetries(\"Unable to render the page. Try increasing timeout\")\n\n        html = HTML(url=self.url, html=content.encode(DEFAULT_ENCODING), default_encoding=DEFAULT_ENCODING)\n        self.__dict__.update(html.__dict__)\n        self.page = page\n        return result\n\n    async def arender(self, retries: int = 8, script: str = None, wait: float = 0.2, scrolldown=False, sleep: int = 0, reload: bool = True, timeout: Union[float, int] = 8.0, keep_page: bool = False, cookies: list = [{}], send_cookies_session: bool = False):\n        \"\"\" Async version of render. Takes same parameters. \"\"\"\n\n        self.browser = await self.session.browser\n        content = None\n\n        # Automatically set Reload to False, if example URL is being used.\n        if self.url == DEFAULT_URL:\n            reload = False\n\n        if send_cookies_session:\n           cookies = self._convert_cookiesjar_to_render()\n\n        for _ in range(retries):\n            if not content:\n                try:\n\n                    content, result, page = await self._async_render(url=self.url, script=script, sleep=sleep, wait=wait, content=self.html, reload=reload, scrolldown=scrolldown, timeout=timeout, keep_page=keep_page, cookies=cookies)\n                except TypeError:\n                    pass\n            else:\n                break\n\n        if not content:\n            raise MaxRetries(\"Unable to render the page. Try increasing timeout\")\n\n        html = HTML(url=self.url, html=content.encode(DEFAULT_ENCODING), default_encoding=DEFAULT_ENCODING)\n        self.__dict__.update(html.__dict__)\n        self.page = page\n        return result\n\n\nclass HTMLResponse(requests.Response):\n    \"\"\"An HTML-enabled :class:`requests.Response <requests.Response>` object.\n    Effectively the same, but with an intelligent ``.html`` property added.\n    \"\"\"\n\n    def __init__(self, session: Union['HTMLSession', 'AsyncHTMLSession']) -> None:\n        super(HTMLResponse, self).__init__()\n        self._html = None  # type: HTML\n        self.session = session\n\n    @property\n    def html(self) -> HTML:\n        if not self._html:\n            self._html = HTML(session=self.session, url=self.url, html=self.content, default_encoding=self.encoding)\n\n        return self._html\n\n    @classmethod\n    def _from_response(cls, response, session: Union['HTMLSession', 'AsyncHTMLSession']):\n        html_r = cls(session=session)\n        html_r.__dict__.update(response.__dict__)\n        return html_r\n\n\ndef user_agent(style=None) -> _UserAgent:\n    \"\"\"Returns an apparently legit user-agent, if not requested one of a specific\n    style. Defaults to a Chrome-style User-Agent.\n    \"\"\"\n    global useragent\n    if (not useragent) and style:\n        useragent = UserAgent()\n\n    return useragent[style] if style else DEFAULT_USER_AGENT\n\n\ndef _get_first_or_list(l, first=False):\n    if first:\n        try:\n            return l[0]\n        except IndexError:\n            return None\n    else:\n        return l\n\n\nclass BaseSession(requests.Session):\n    \"\"\" A consumable session, for cookie persistence and connection pooling,\n    amongst other things.\n    \"\"\"\n\n    def __init__(self, mock_browser : bool = True, verify : bool = True,\n                 browser_args : list = ['--no-sandbox']):\n        super().__init__()\n\n        # Mock a web browser's user agent.\n        if mock_browser:\n            self.headers['User-Agent'] = user_agent()\n\n        self.hooks['response'].append(self.response_hook)\n        self.verify = verify\n\n        self.__browser_args = browser_args\n\n\n    def response_hook(self, response, **kwargs) -> HTMLResponse:\n        \"\"\" Change response encoding and replace it by a HTMLResponse. \"\"\"\n        if not response.encoding:\n            response.encoding = DEFAULT_ENCODING\n        return HTMLResponse._from_response(response, self)\n\n    @property\n    async def browser(self):\n        if not hasattr(self, \"_browser\"):\n            self._browser = await pyppeteer.launch(ignoreHTTPSErrors=not(self.verify), headless=True, args=self.__browser_args)\n\n        return self._browser\n\n\nclass HTMLSession(BaseSession):\n\n    def __init__(self, **kwargs):\n        super(HTMLSession, self).__init__(**kwargs)\n\n    @property\n    def browser(self):\n        if not hasattr(self, \"_browser\"):\n            self.loop = asyncio.get_event_loop()\n            if self.loop.is_running():\n                raise RuntimeError(\"Cannot use HTMLSession within an existing event loop. Use AsyncHTMLSession instead.\")\n            self._browser = self.loop.run_until_complete(super().browser)\n        return self._browser\n\n    def close(self):\n        \"\"\" If a browser was created close it first. \"\"\"\n        if hasattr(self, \"_browser\"):\n            self.loop.run_until_complete(self._browser.close())\n        super().close()\n\n\nclass AsyncHTMLSession(BaseSession):\n    \"\"\" An async consumable session. \"\"\"\n\n    def __init__(self, loop=None, workers=None,\n                 mock_browser: bool = True, *args, **kwargs):\n        \"\"\" Set or create an event loop and a thread pool.\n\n            :param loop: Asyncio loop to use.\n            :param workers: Amount of threads to use for executing async calls.\n                If not pass it will default to the number of processors on the\n                machine, multiplied by 5. \"\"\"\n        super().__init__(*args, **kwargs)\n\n        self.loop = loop or asyncio.get_event_loop()\n        self.thread_pool = ThreadPoolExecutor(max_workers=workers)\n\n    def request(self, *args, **kwargs):\n        \"\"\" Partial original request func and run it in a thread. \"\"\"\n        func = partial(super().request, *args, **kwargs)\n        return self.loop.run_in_executor(self.thread_pool, func)\n\n    async def close(self):\n        \"\"\" If a browser was created close it first. \"\"\"\n        if hasattr(self, \"_browser\"):\n            await self._browser.close()\n        super().close()\n\n    def run(self, *coros):\n        \"\"\" Pass in all the coroutines you want to run, it will wrap each one\n            in a task, run it and wait for the result. Return a list with all\n            results, this is returned in the same order coros are passed in. \"\"\"\n        tasks = [\n            asyncio.ensure_future(coro()) for coro in coros\n        ]\n        done, _ = self.loop.run_until_complete(asyncio.wait(tasks))\n        return [t.result() for t in done]\n"
    },
    {
        "repository": "psf/requests-html",
        "file_name": "setup.py",
        "content": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n# Note: To use the 'upload' functionality of this file, you must:\n#   $ pip install twine\n\nimport io\nimport os\nimport sys\nfrom shutil import rmtree\n\nfrom setuptools import setup, Command\n\n# Package meta-data.\nNAME = 'requests-html'\nDESCRIPTION = 'HTML Parsing for Humans.'\nURL = 'https://github.com/psf/requests-html'\nEMAIL = 'me@kennethreitz.org'\nAUTHOR = 'Kenneth Reitz'\nVERSION = '0.10.0'\n\n# What packages are required for this module to be executed?\nREQUIRED = [\n    'requests', 'pyquery', 'fake-useragent', 'parse', 'beautifulsoup4', 'w3lib', 'pyppeteer>=0.0.14'\n]\n\n# The rest you shouldn't have to touch too much :)\n# ------------------------------------------------\n# Except, perhaps the License and Trove Classifiers!\n# If you do change the License, remember to change the Trove Classifier for that!\n\nhere = os.path.abspath(os.path.dirname(__file__))\n\n# Import the README and use it as the long-description.\n# Note: this will only work if 'README.rst' is present in your MANIFEST.in file!\nwith io.open(os.path.join(here, 'README.rst'), encoding='utf-8') as f:\n    long_description = '\\n' + f.read()\n\nclass UploadCommand(Command):\n    \"\"\"Support setup.py upload.\"\"\"\n\n    description = 'Build and publish the package.'\n    user_options = []\n\n    @staticmethod\n    def status(s):\n        \"\"\"Prints things in bold.\"\"\"\n        print('\\033[1m{0}\\033[0m'.format(s))\n\n    def initialize_options(self):\n        pass\n\n    def finalize_options(self):\n        pass\n\n    def run(self):\n        try:\n            self.status('Removing previous builds\u2026')\n            rmtree(os.path.join(here, 'dist'))\n        except OSError:\n            pass\n\n        self.status('Building Source and Wheel (universal) distribution\u2026')\n        os.system('{0} setup.py sdist bdist_wheel --universal'.format(sys.executable))\n\n        self.status('Uploading the package to PyPi via Twine\u2026')\n        os.system('twine upload dist/*')\n\n        self.status('Publishing git tags\u2026')\n        os.system('git tag v{0}'.format(VERSION))\n        os.system('git push --tags')\n\n        sys.exit()\n\n\n# Where the magic happens:\nsetup(\n    name=NAME,\n    version=VERSION,\n    description=DESCRIPTION,\n    long_description=long_description,\n    author=AUTHOR,\n    author_email=EMAIL,\n    url=URL,\n    python_requires='>=3.6.0',\n    # If your package is a single module, use this instead of 'packages':\n    py_modules=['requests_html'],\n\n    # entry_points={\n    #     'console_scripts': ['mycli=mymodule:cli'],\n    # },\n    install_requires=REQUIRED,\n    include_package_data=True,\n    license='MIT',\n    classifiers=[\n        # Trove classifiers\n        # Full list: https://pypi.python.org/pypi?%3Aaction=list_classifiers\n        'License :: OSI Approved :: MIT License',\n        'Programming Language :: Python',\n        'Programming Language :: Python :: 3.6',\n        'Programming Language :: Python :: Implementation :: CPython',\n        'Programming Language :: Python :: Implementation :: PyPy'\n    ],\n    # $ setup.py publish support.\n    cmdclass={\n        'upload': UploadCommand,\n    },\n)\n"
    },
    {
        "repository": "pytorch/pytorch",
        "file_name": "setup.py",
        "content": "# Welcome to the PyTorch setup.py.\n#\n# Environment variables you are probably interested in:\n#\n#   DEBUG\n#     build with -O0 and -g (debug symbols)\n#\n#   REL_WITH_DEB_INFO\n#     build with optimizations and -g (debug symbols)\n#\n#   USE_CUSTOM_DEBINFO=\"path/to/file1.cpp;path/to/file2.cpp\"\n#     build with debug info only for specified files\n#\n#   MAX_JOBS\n#     maximum number of compile jobs we should use to compile your code\n#\n#   USE_CUDA=0\n#     disables CUDA build\n#\n#   CFLAGS\n#     flags to apply to both C and C++ files to be compiled (a quirk of setup.py\n#     which we have faithfully adhered to in our build system is that CFLAGS\n#     also applies to C++ files (unless CXXFLAGS is set), in contrast to the\n#     default behavior of autogoo and cmake build systems.)\n#\n#   CC\n#     the C/C++ compiler to use\n#\n# Environment variables for feature toggles:\n#\n#   DEBUG_CUDA=1\n#     if used in conjunction with DEBUG or REL_WITH_DEB_INFO, will also\n#     build CUDA kernels with -lineinfo --source-in-ptx.  Note that\n#     on CUDA 12 this may cause nvcc to OOM, so this is disabled by default.\n\n#   USE_CUDNN=0\n#     disables the cuDNN build\n#\n#   USE_CUSPARSELT=0\n#     disables the cuSPARSELt build\n#\n#   USE_FBGEMM=0\n#     disables the FBGEMM build\n#\n#   USE_KINETO=0\n#     disables usage of libkineto library for profiling\n#\n#   USE_NUMPY=0\n#     disables the NumPy build\n#\n#   BUILD_TEST=0\n#     disables the test build\n#\n#   USE_MKLDNN=0\n#     disables use of MKLDNN\n#\n#   USE_MKLDNN_ACL\n#     enables use of Compute Library backend for MKLDNN on Arm;\n#     USE_MKLDNN must be explicitly enabled.\n#\n#   MKLDNN_CPU_RUNTIME\n#     MKL-DNN threading mode: TBB or OMP (default)\n#\n#   USE_STATIC_MKL\n#     Prefer to link with MKL statically - Unix only\n#   USE_ITT=0\n#     disable use of Intel(R) VTune Profiler's ITT functionality\n#\n#   USE_NNPACK=0\n#     disables NNPACK build\n#\n#   USE_QNNPACK=0\n#     disables QNNPACK build (quantized 8-bit operators)\n#\n#   USE_DISTRIBUTED=0\n#     disables distributed (c10d, gloo, mpi, etc.) build\n#\n#   USE_TENSORPIPE=0\n#     disables distributed Tensorpipe backend build\n#\n#   USE_GLOO=0\n#     disables distributed gloo backend build\n#\n#   USE_MPI=0\n#     disables distributed MPI backend build\n#\n#   USE_SYSTEM_NCCL=0\n#     disables use of system-wide nccl (we will use our submoduled\n#     copy in third_party/nccl)\n#\n#   BUILD_CAFFE2_OPS=0\n#     disable Caffe2 operators build\n#\n#   BUILD_CAFFE2=0\n#     disable Caffe2 build\n#\n#   USE_IBVERBS\n#     toggle features related to distributed support\n#\n#   USE_OPENCV\n#     enables use of OpenCV for additional operators\n#\n#   USE_OPENMP=0\n#     disables use of OpenMP for parallelization\n#\n#   USE_FFMPEG\n#     enables use of ffmpeg for additional operators\n#\n#   USE_FLASH_ATTENTION=0\n#     disables building flash attention for scaled dot product attention\n#\n#   USE_MEM_EFF_ATTENTION=0\n#    disables building memory efficient attention for scaled dot product attention\n#\n#   USE_LEVELDB\n#     enables use of LevelDB for storage\n#\n#   USE_LMDB\n#     enables use of LMDB for storage\n#\n#   BUILD_BINARY\n#     enables the additional binaries/ build\n#\n#   ATEN_AVX512_256=TRUE\n#     ATen AVX2 kernels can use 32 ymm registers, instead of the default 16.\n#     This option can be used if AVX512 doesn't perform well on a machine.\n#     The FBGEMM library also uses AVX512_256 kernels on Xeon D processors,\n#     but it also has some (optimized) assembly code.\n#\n#   PYTORCH_BUILD_VERSION\n#   PYTORCH_BUILD_NUMBER\n#     specify the version of PyTorch, rather than the hard-coded version\n#     in this file; used when we're building binaries for distribution\n#\n#   TORCH_CUDA_ARCH_LIST\n#     specify which CUDA architectures to build for.\n#     ie `TORCH_CUDA_ARCH_LIST=\"6.0;7.0\"`\n#     These are not CUDA versions, instead, they specify what\n#     classes of NVIDIA hardware we should generate PTX for.\n#\n#   PYTORCH_ROCM_ARCH\n#     specify which AMD GPU targets to build for.\n#     ie `PYTORCH_ROCM_ARCH=\"gfx900;gfx906\"`\n#\n#   ONNX_NAMESPACE\n#     specify a namespace for ONNX built here rather than the hard-coded\n#     one in this file; needed to build with other frameworks that share ONNX.\n#\n#   BLAS\n#     BLAS to be used by Caffe2. Can be MKL, Eigen, ATLAS, FlexiBLAS, or OpenBLAS. If set\n#     then the build will fail if the requested BLAS is not found, otherwise\n#     the BLAS will be chosen based on what is found on your system.\n#\n#   MKL_THREADING\n#     MKL threading mode: SEQ, TBB or OMP (default)\n#\n#   USE_REDIS\n#     Whether to use Redis for distributed workflows (Linux only)\n#\n#   USE_ZSTD\n#     Enables use of ZSTD, if the libraries are found\n#\n# Environment variables we respect (these environment variables are\n# conventional and are often understood/set by other software.)\n#\n#   CUDA_HOME (Linux/OS X)\n#   CUDA_PATH (Windows)\n#     specify where CUDA is installed; usually /usr/local/cuda or\n#     /usr/local/cuda-x.y\n#   CUDAHOSTCXX\n#     specify a different compiler than the system one to use as the CUDA\n#     host compiler for nvcc.\n#\n#   CUDA_NVCC_EXECUTABLE\n#     Specify a NVCC to use. This is used in our CI to point to a cached nvcc\n#\n#   CUDNN_LIB_DIR\n#   CUDNN_INCLUDE_DIR\n#   CUDNN_LIBRARY\n#     specify where cuDNN is installed\n#\n#   MIOPEN_LIB_DIR\n#   MIOPEN_INCLUDE_DIR\n#   MIOPEN_LIBRARY\n#     specify where MIOpen is installed\n#\n#   NCCL_ROOT\n#   NCCL_LIB_DIR\n#   NCCL_INCLUDE_DIR\n#     specify where nccl is installed\n#\n#   NVTOOLSEXT_PATH (Windows only)\n#     specify where nvtoolsext is installed\n#\n#   ACL_ROOT_DIR\n#     specify where Compute Library is installed\n#\n#   LIBRARY_PATH\n#   LD_LIBRARY_PATH\n#     we will search for libraries in these paths\n#\n#   ATEN_THREADING\n#     ATen parallel backend to use for intra- and inter-op parallelism\n#     possible values:\n#       OMP - use OpenMP for intra-op and native backend for inter-op tasks\n#       NATIVE - use native thread pool for both intra- and inter-op tasks\n#       TBB - using TBB for intra- and native thread pool for inter-op parallelism\n#\n#   USE_TBB\n#      enable TBB support\n#\n#   USE_SYSTEM_TBB\n#      Use system-provided Intel TBB.\n#\n#   USE_SYSTEM_LIBS (work in progress)\n#      Use system-provided libraries to satisfy the build dependencies.\n#      When turned on, the following cmake variables will be toggled as well:\n#        USE_SYSTEM_CPUINFO=ON USE_SYSTEM_SLEEF=ON BUILD_CUSTOM_PROTOBUF=OFF\n#\n#   USE_MIMALLOC\n#      Static link mimalloc into C10, and use mimalloc in alloc_cpu & alloc_free.\n#      By default, It is only enabled on Windows.\n\nimport sys\n\nif sys.platform == \"win32\" and sys.maxsize.bit_length() == 31:\n    print(\n        \"32-bit Windows Python runtime is not supported. Please switch to 64-bit Python.\"\n    )\n    sys.exit(-1)\n\nimport platform\n\npython_min_version = (3, 8, 0)\npython_min_version_str = \".\".join(map(str, python_min_version))\nif sys.version_info < python_min_version:\n    print(\n        f\"You are using Python {platform.python_version()}. Python >={python_min_version_str} is required.\"\n    )\n    sys.exit(-1)\n\nimport filecmp\nimport glob\nimport importlib\nimport json\nimport os\nimport shutil\nimport subprocess\nimport sysconfig\nimport time\nfrom collections import defaultdict\n\nimport setuptools.command.build_ext\nimport setuptools.command.install\nimport setuptools.command.sdist\nfrom setuptools import Extension, find_packages, setup\nfrom setuptools.dist import Distribution\n\nfrom tools.build_pytorch_libs import build_caffe2\nfrom tools.generate_torch_version import get_torch_version\nfrom tools.setup_helpers.cmake import CMake\nfrom tools.setup_helpers.env import build_type, IS_DARWIN, IS_LINUX, IS_WINDOWS\n\n################################################################################\n# Parameters parsed from environment\n################################################################################\n\nVERBOSE_SCRIPT = True\nRUN_BUILD_DEPS = True\n# see if the user passed a quiet flag to setup.py arguments and respect\n# that in our parts of the build\nEMIT_BUILD_WARNING = False\nRERUN_CMAKE = False\nCMAKE_ONLY = False\nfiltered_args = []\nfor i, arg in enumerate(sys.argv):\n    if arg == \"--cmake\":\n        RERUN_CMAKE = True\n        continue\n    if arg == \"--cmake-only\":\n        # Stop once cmake terminates. Leave users a chance to adjust build\n        # options.\n        CMAKE_ONLY = True\n        continue\n    if arg == \"rebuild\" or arg == \"build\":\n        arg = \"build\"  # rebuild is gone, make it build\n        EMIT_BUILD_WARNING = True\n    if arg == \"--\":\n        filtered_args += sys.argv[i:]\n        break\n    if arg == \"-q\" or arg == \"--quiet\":\n        VERBOSE_SCRIPT = False\n    if arg in [\"clean\", \"egg_info\", \"sdist\"]:\n        RUN_BUILD_DEPS = False\n    filtered_args.append(arg)\nsys.argv = filtered_args\n\nif VERBOSE_SCRIPT:\n\n    def report(*args):\n        print(*args)\n\nelse:\n\n    def report(*args):\n        pass\n\n    # Make distutils respect --quiet too\n    setuptools.distutils.log.warn = report\n\n# Constant known variables used throughout this file\ncwd = os.path.dirname(os.path.abspath(__file__))\nlib_path = os.path.join(cwd, \"torch\", \"lib\")\nthird_party_path = os.path.join(cwd, \"third_party\")\ncaffe2_build_dir = os.path.join(cwd, \"build\")\n\n# CMAKE: full path to python library\nif IS_WINDOWS:\n    cmake_python_library = \"{}/libs/python{}.lib\".format(\n        sysconfig.get_config_var(\"prefix\"), sysconfig.get_config_var(\"VERSION\")\n    )\n    # Fix virtualenv builds\n    if not os.path.exists(cmake_python_library):\n        cmake_python_library = \"{}/libs/python{}.lib\".format(\n            sys.base_prefix, sysconfig.get_config_var(\"VERSION\")\n        )\nelse:\n    cmake_python_library = \"{}/{}\".format(\n        sysconfig.get_config_var(\"LIBDIR\"), sysconfig.get_config_var(\"INSTSONAME\")\n    )\ncmake_python_include_dir = sysconfig.get_path(\"include\")\n\n\n################################################################################\n# Version, create_version_file, and package_name\n################################################################################\npackage_name = os.getenv(\"TORCH_PACKAGE_NAME\", \"torch\")\npackage_type = os.getenv(\"PACKAGE_TYPE\", \"wheel\")\nversion = get_torch_version()\nreport(f\"Building wheel {package_name}-{version}\")\n\ncmake = CMake()\n\n\ndef get_submodule_folders():\n    git_modules_path = os.path.join(cwd, \".gitmodules\")\n    default_modules_path = [\n        os.path.join(third_party_path, name)\n        for name in [\n            \"gloo\",\n            \"cpuinfo\",\n            \"tbb\",\n            \"onnx\",\n            \"foxi\",\n            \"QNNPACK\",\n            \"fbgemm\",\n            \"cutlass\",\n        ]\n    ]\n    if not os.path.exists(git_modules_path):\n        return default_modules_path\n    with open(git_modules_path) as f:\n        return [\n            os.path.join(cwd, line.split(\"=\", 1)[1].strip())\n            for line in f.readlines()\n            if line.strip().startswith(\"path\")\n        ]\n\n\ndef check_submodules():\n    def check_for_files(folder, files):\n        if not any(os.path.exists(os.path.join(folder, f)) for f in files):\n            report(\"Could not find any of {} in {}\".format(\", \".join(files), folder))\n            report(\"Did you run 'git submodule update --init --recursive'?\")\n            sys.exit(1)\n\n    def not_exists_or_empty(folder):\n        return not os.path.exists(folder) or (\n            os.path.isdir(folder) and len(os.listdir(folder)) == 0\n        )\n\n    if bool(os.getenv(\"USE_SYSTEM_LIBS\", False)):\n        return\n    folders = get_submodule_folders()\n    # If none of the submodule folders exists, try to initialize them\n    if all(not_exists_or_empty(folder) for folder in folders):\n        try:\n            print(\" --- Trying to initialize submodules\")\n            start = time.time()\n            subprocess.check_call(\n                [\"git\", \"submodule\", \"update\", \"--init\", \"--recursive\"], cwd=cwd\n            )\n            end = time.time()\n            print(f\" --- Submodule initialization took {end - start:.2f} sec\")\n        except Exception:\n            print(\" --- Submodule initalization failed\")\n            print(\"Please run:\\n\\tgit submodule update --init --recursive\")\n            sys.exit(1)\n    for folder in folders:\n        check_for_files(\n            folder,\n            [\n                \"CMakeLists.txt\",\n                \"Makefile\",\n                \"setup.py\",\n                \"LICENSE\",\n                \"LICENSE.md\",\n                \"LICENSE.txt\",\n            ],\n        )\n    check_for_files(\n        os.path.join(third_party_path, \"fbgemm\", \"third_party\", \"asmjit\"),\n        [\"CMakeLists.txt\"],\n    )\n    check_for_files(\n        os.path.join(third_party_path, \"onnx\", \"third_party\", \"benchmark\"),\n        [\"CMakeLists.txt\"],\n    )\n\n\n# Windows has very bad support for symbolic links.\n# Instead of using symlinks, we're going to copy files over\ndef mirror_files_into_torchgen():\n    # (new_path, orig_path)\n    # Directories are OK and are recursively mirrored.\n    paths = [\n        (\n            \"torchgen/packaged/ATen/native/native_functions.yaml\",\n            \"aten/src/ATen/native/native_functions.yaml\",\n        ),\n        (\"torchgen/packaged/ATen/native/tags.yaml\", \"aten/src/ATen/native/tags.yaml\"),\n        (\"torchgen/packaged/ATen/templates\", \"aten/src/ATen/templates\"),\n        (\"torchgen/packaged/autograd\", \"tools/autograd\"),\n        (\"torchgen/packaged/autograd/templates\", \"tools/autograd/templates\"),\n    ]\n    for new_path, orig_path in paths:\n        # Create the dirs involved in new_path if they don't exist\n        if not os.path.exists(new_path):\n            os.makedirs(os.path.dirname(new_path), exist_ok=True)\n\n        # Copy the files from the orig location to the new location\n        if os.path.isfile(orig_path):\n            shutil.copyfile(orig_path, new_path)\n            continue\n        if os.path.isdir(orig_path):\n            if os.path.exists(new_path):\n                # copytree fails if the tree exists already, so remove it.\n                shutil.rmtree(new_path)\n            shutil.copytree(orig_path, new_path)\n            continue\n        raise RuntimeError(\"Check the file paths in `mirror_files_into_torchgen()`\")\n\n\n# all the work we need to do _before_ setup runs\ndef build_deps():\n    report(\"-- Building version \" + version)\n\n    check_submodules()\n    check_pydep(\"yaml\", \"pyyaml\")\n\n    build_caffe2(\n        version=version,\n        cmake_python_library=cmake_python_library,\n        build_python=True,\n        rerun_cmake=RERUN_CMAKE,\n        cmake_only=CMAKE_ONLY,\n        cmake=cmake,\n    )\n\n    if CMAKE_ONLY:\n        report(\n            'Finished running cmake. Run \"ccmake build\" or '\n            '\"cmake-gui build\" to adjust build options and '\n            '\"python setup.py install\" to build.'\n        )\n        sys.exit()\n\n    # Use copies instead of symbolic files.\n    # Windows has very poor support for them.\n    sym_files = [\n        \"tools/shared/_utils_internal.py\",\n        \"torch/utils/benchmark/utils/valgrind_wrapper/callgrind.h\",\n        \"torch/utils/benchmark/utils/valgrind_wrapper/valgrind.h\",\n    ]\n    orig_files = [\n        \"torch/_utils_internal.py\",\n        \"third_party/valgrind-headers/callgrind.h\",\n        \"third_party/valgrind-headers/valgrind.h\",\n    ]\n    for sym_file, orig_file in zip(sym_files, orig_files):\n        same = False\n        if os.path.exists(sym_file):\n            if filecmp.cmp(sym_file, orig_file):\n                same = True\n            else:\n                os.remove(sym_file)\n        if not same:\n            shutil.copyfile(orig_file, sym_file)\n\n\n################################################################################\n# Building dependent libraries\n################################################################################\n\nmissing_pydep = \"\"\"\nMissing build dependency: Unable to `import {importname}`.\nPlease install it via `conda install {module}` or `pip install {module}`\n\"\"\".strip()\n\n\ndef check_pydep(importname, module):\n    try:\n        importlib.import_module(importname)\n    except ImportError as e:\n        raise RuntimeError(\n            missing_pydep.format(importname=importname, module=module)\n        ) from e\n\n\nclass build_ext(setuptools.command.build_ext.build_ext):\n    # Copy libiomp5.dylib inside the wheel package on OS X\n    def _embed_libiomp(self):\n        lib_dir = os.path.join(self.build_lib, \"torch\", \"lib\")\n        libtorch_cpu_path = os.path.join(lib_dir, \"libtorch_cpu.dylib\")\n        if not os.path.exists(libtorch_cpu_path):\n            return\n        # Parse libtorch_cpu load commands\n        otool_cmds = (\n            subprocess.check_output([\"otool\", \"-l\", libtorch_cpu_path])\n            .decode(\"utf-8\")\n            .split(\"\\n\")\n        )\n        rpaths, libs = [], []\n        for idx, line in enumerate(otool_cmds):\n            if line.strip() == \"cmd LC_LOAD_DYLIB\":\n                lib_name = otool_cmds[idx + 2].strip()\n                assert lib_name.startswith(\"name \")\n                libs.append(lib_name.split(\" \", 1)[1].rsplit(\"(\", 1)[0][:-1])\n\n            if line.strip() == \"cmd LC_RPATH\":\n                rpath = otool_cmds[idx + 2].strip()\n                assert rpath.startswith(\"path \")\n                rpaths.append(rpath.split(\" \", 1)[1].rsplit(\"(\", 1)[0][:-1])\n\n        omp_lib_name = \"libiomp5.dylib\"\n        if os.path.join(\"@rpath\", omp_lib_name) not in libs:\n            return\n\n        # Copy libiomp5 from rpath locations\n        for rpath in rpaths:\n            source_lib = os.path.join(rpath, omp_lib_name)\n            if not os.path.exists(source_lib):\n                continue\n            target_lib = os.path.join(self.build_lib, \"torch\", \"lib\", omp_lib_name)\n            self.copy_file(source_lib, target_lib)\n            break\n\n    def run(self):\n        # Report build options. This is run after the build completes so # `CMakeCache.txt` exists and we can get an\n        # accurate report on what is used and what is not.\n        cmake_cache_vars = defaultdict(lambda: False, cmake.get_cmake_cache_variables())\n        if cmake_cache_vars[\"USE_NUMPY\"]:\n            report(\"-- Building with NumPy bindings\")\n        else:\n            report(\"-- NumPy not found\")\n        if cmake_cache_vars[\"USE_CUDNN\"]:\n            report(\n                \"-- Detected cuDNN at \"\n                + cmake_cache_vars[\"CUDNN_LIBRARY\"]\n                + \", \"\n                + cmake_cache_vars[\"CUDNN_INCLUDE_DIR\"]\n            )\n        else:\n            report(\"-- Not using cuDNN\")\n        if cmake_cache_vars[\"USE_CUDA\"]:\n            report(\"-- Detected CUDA at \" + cmake_cache_vars[\"CUDA_TOOLKIT_ROOT_DIR\"])\n        else:\n            report(\"-- Not using CUDA\")\n        if cmake_cache_vars[\"USE_MKLDNN\"]:\n            report(\"-- Using MKLDNN\")\n            if cmake_cache_vars[\"USE_MKLDNN_ACL\"]:\n                report(\"-- Using Compute Library for the Arm architecture with MKLDNN\")\n            else:\n                report(\n                    \"-- Not using Compute Library for the Arm architecture with MKLDNN\"\n                )\n            if cmake_cache_vars[\"USE_MKLDNN_CBLAS\"]:\n                report(\"-- Using CBLAS in MKLDNN\")\n            else:\n                report(\"-- Not using CBLAS in MKLDNN\")\n        else:\n            report(\"-- Not using MKLDNN\")\n        if cmake_cache_vars[\"USE_NCCL\"] and cmake_cache_vars[\"USE_SYSTEM_NCCL\"]:\n            report(\n                \"-- Using system provided NCCL library at {}, {}\".format(\n                    cmake_cache_vars[\"NCCL_LIBRARIES\"],\n                    cmake_cache_vars[\"NCCL_INCLUDE_DIRS\"],\n                )\n            )\n        elif cmake_cache_vars[\"USE_NCCL\"]:\n            report(\"-- Building NCCL library\")\n        else:\n            report(\"-- Not using NCCL\")\n        if cmake_cache_vars[\"USE_DISTRIBUTED\"]:\n            if IS_WINDOWS:\n                report(\"-- Building without distributed package\")\n            else:\n                report(\"-- Building with distributed package: \")\n                report(\n                    \"  -- USE_TENSORPIPE={}\".format(cmake_cache_vars[\"USE_TENSORPIPE\"])\n                )\n                report(\"  -- USE_GLOO={}\".format(cmake_cache_vars[\"USE_GLOO\"]))\n                report(\"  -- USE_MPI={}\".format(cmake_cache_vars[\"USE_OPENMPI\"]))\n        else:\n            report(\"-- Building without distributed package\")\n        if cmake_cache_vars[\"STATIC_DISPATCH_BACKEND\"]:\n            report(\n                \"-- Using static dispatch with backend {}\".format(\n                    cmake_cache_vars[\"STATIC_DISPATCH_BACKEND\"]\n                )\n            )\n        if cmake_cache_vars[\"USE_LIGHTWEIGHT_DISPATCH\"]:\n            report(\"-- Using lightweight dispatch\")\n        if cmake_cache_vars[\"BUILD_EXECUTORCH\"]:\n            report(\"-- Building Executorch\")\n\n        if cmake_cache_vars[\"USE_ITT\"]:\n            report(\"-- Using ITT\")\n        else:\n            report(\"-- Not using ITT\")\n\n        # Do not use clang to compile extensions if `-fstack-clash-protection` is defined\n        # in system CFLAGS\n        c_flags = str(os.getenv(\"CFLAGS\", \"\"))\n        if (\n            IS_LINUX\n            and \"-fstack-clash-protection\" in c_flags\n            and \"clang\" in os.environ.get(\"CC\", \"\")\n        ):\n            os.environ[\"CC\"] = str(os.environ[\"CC\"])\n\n        # It's an old-style class in Python 2.7...\n        setuptools.command.build_ext.build_ext.run(self)\n\n        if IS_DARWIN and package_type != \"conda\":\n            self._embed_libiomp()\n\n        # Copy the essential export library to compile C++ extensions.\n        if IS_WINDOWS:\n            build_temp = self.build_temp\n\n            ext_filename = self.get_ext_filename(\"_C\")\n            lib_filename = \".\".join(ext_filename.split(\".\")[:-1]) + \".lib\"\n\n            export_lib = os.path.join(\n                build_temp, \"torch\", \"csrc\", lib_filename\n            ).replace(\"\\\\\", \"/\")\n\n            build_lib = self.build_lib\n\n            target_lib = os.path.join(build_lib, \"torch\", \"lib\", \"_C.lib\").replace(\n                \"\\\\\", \"/\"\n            )\n\n            # Create \"torch/lib\" directory if not exists.\n            # (It is not created yet in \"develop\" mode.)\n            target_dir = os.path.dirname(target_lib)\n            if not os.path.exists(target_dir):\n                os.makedirs(target_dir)\n\n            self.copy_file(export_lib, target_lib)\n\n    def build_extensions(self):\n        self.create_compile_commands()\n        # The caffe2 extensions are created in\n        # tmp_install/lib/pythonM.m/site-packages/caffe2/python/\n        # and need to be copied to build/lib.linux.... , which will be a\n        # platform dependent build folder created by the \"build\" command of\n        # setuptools. Only the contents of this folder are installed in the\n        # \"install\" command by default.\n        # We only make this copy for Caffe2's pybind extensions\n        caffe2_pybind_exts = [\n            \"caffe2.python.caffe2_pybind11_state\",\n            \"caffe2.python.caffe2_pybind11_state_gpu\",\n            \"caffe2.python.caffe2_pybind11_state_hip\",\n        ]\n        i = 0\n        while i < len(self.extensions):\n            ext = self.extensions[i]\n            if ext.name not in caffe2_pybind_exts:\n                i += 1\n                continue\n            fullname = self.get_ext_fullname(ext.name)\n            filename = self.get_ext_filename(fullname)\n            report(f\"\\nCopying extension {ext.name}\")\n\n            relative_site_packages = (\n                sysconfig.get_path(\"purelib\")\n                .replace(sysconfig.get_path(\"data\"), \"\")\n                .lstrip(os.path.sep)\n            )\n            src = os.path.join(\"torch\", relative_site_packages, filename)\n            if not os.path.exists(src):\n                report(f\"{src} does not exist\")\n                del self.extensions[i]\n            else:\n                dst = os.path.join(os.path.realpath(self.build_lib), filename)\n                report(f\"Copying {ext.name} from {src} to {dst}\")\n                dst_dir = os.path.dirname(dst)\n                if not os.path.exists(dst_dir):\n                    os.makedirs(dst_dir)\n                self.copy_file(src, dst)\n                i += 1\n\n        # Copy functorch extension\n        for i, ext in enumerate(self.extensions):\n            if ext.name != \"functorch._C\":\n                continue\n            fullname = self.get_ext_fullname(ext.name)\n            filename = self.get_ext_filename(fullname)\n            fileext = os.path.splitext(filename)[1]\n            src = os.path.join(os.path.dirname(filename), \"functorch\" + fileext)\n            dst = os.path.join(os.path.realpath(self.build_lib), filename)\n            if os.path.exists(src):\n                report(f\"Copying {ext.name} from {src} to {dst}\")\n                dst_dir = os.path.dirname(dst)\n                if not os.path.exists(dst_dir):\n                    os.makedirs(dst_dir)\n                self.copy_file(src, dst)\n\n        setuptools.command.build_ext.build_ext.build_extensions(self)\n\n    def get_outputs(self):\n        outputs = setuptools.command.build_ext.build_ext.get_outputs(self)\n        outputs.append(os.path.join(self.build_lib, \"caffe2\"))\n        report(f\"setup.py::get_outputs returning {outputs}\")\n        return outputs\n\n    def create_compile_commands(self):\n        def load(filename):\n            with open(filename) as f:\n                return json.load(f)\n\n        ninja_files = glob.glob(\"build/*compile_commands.json\")\n        cmake_files = glob.glob(\"torch/lib/build/*/compile_commands.json\")\n        all_commands = [entry for f in ninja_files + cmake_files for entry in load(f)]\n\n        # cquery does not like c++ compiles that start with gcc.\n        # It forgets to include the c++ header directories.\n        # We can work around this by replacing the gcc calls that python\n        # setup.py generates with g++ calls instead\n        for command in all_commands:\n            if command[\"command\"].startswith(\"gcc \"):\n                command[\"command\"] = \"g++ \" + command[\"command\"][4:]\n\n        new_contents = json.dumps(all_commands, indent=2)\n        contents = \"\"\n        if os.path.exists(\"compile_commands.json\"):\n            with open(\"compile_commands.json\") as f:\n                contents = f.read()\n        if contents != new_contents:\n            with open(\"compile_commands.json\", \"w\") as f:\n                f.write(new_contents)\n\n\nclass concat_license_files:\n    \"\"\"Merge LICENSE and LICENSES_BUNDLED.txt as a context manager\n\n    LICENSE is the main PyTorch license, LICENSES_BUNDLED.txt is auto-generated\n    from all the licenses found in ./third_party/. We concatenate them so there\n    is a single license file in the sdist and wheels with all of the necessary\n    licensing info.\n    \"\"\"\n\n    def __init__(self, include_files=False):\n        self.f1 = \"LICENSE\"\n        self.f2 = \"third_party/LICENSES_BUNDLED.txt\"\n        self.include_files = include_files\n\n    def __enter__(self):\n        \"\"\"Concatenate files\"\"\"\n\n        old_path = sys.path\n        sys.path.append(third_party_path)\n        try:\n            from build_bundled import create_bundled\n        finally:\n            sys.path = old_path\n\n        with open(self.f1) as f1:\n            self.bsd_text = f1.read()\n\n        with open(self.f1, \"a\") as f1:\n            f1.write(\"\\n\\n\")\n            create_bundled(\n                os.path.relpath(third_party_path), f1, include_files=self.include_files\n            )\n\n    def __exit__(self, exception_type, exception_value, traceback):\n        \"\"\"Restore content of f1\"\"\"\n        with open(self.f1, \"w\") as f:\n            f.write(self.bsd_text)\n\n\ntry:\n    from wheel.bdist_wheel import bdist_wheel\nexcept ImportError:\n    # This is useful when wheel is not installed and bdist_wheel is not\n    # specified on the command line. If it _is_ specified, parsing the command\n    # line will fail before wheel_concatenate is needed\n    wheel_concatenate = None\nelse:\n    # Need to create the proper LICENSE.txt for the wheel\n    class wheel_concatenate(bdist_wheel):\n        \"\"\"check submodules on sdist to prevent incomplete tarballs\"\"\"\n\n        def run(self):\n            with concat_license_files(include_files=True):\n                super().run()\n\n\nclass install(setuptools.command.install.install):\n    def run(self):\n        super().run()\n\n\nclass clean(setuptools.Command):\n    user_options = []\n\n    def initialize_options(self):\n        pass\n\n    def finalize_options(self):\n        pass\n\n    def run(self):\n        import glob\n        import re\n\n        with open(\".gitignore\") as f:\n            ignores = f.read()\n            pat = re.compile(r\"^#( BEGIN NOT-CLEAN-FILES )?\")\n            for wildcard in filter(None, ignores.split(\"\\n\")):\n                match = pat.match(wildcard)\n                if match:\n                    if match.group(1):\n                        # Marker is found and stop reading .gitignore.\n                        break\n                    # Ignore lines which begin with '#'.\n                else:\n                    # Don't remove absolute paths from the system\n                    wildcard = wildcard.lstrip(\"./\")\n\n                    for filename in glob.glob(wildcard):\n                        try:\n                            os.remove(filename)\n                        except OSError:\n                            shutil.rmtree(filename, ignore_errors=True)\n\n\nclass sdist(setuptools.command.sdist.sdist):\n    def run(self):\n        with concat_license_files():\n            super().run()\n\n\ndef get_cmake_cache_vars():\n    try:\n        return defaultdict(lambda: False, cmake.get_cmake_cache_variables())\n    except FileNotFoundError:\n        # CMakeCache.txt does not exist. Probably running \"python setup.py clean\" over a clean directory.\n        return defaultdict(lambda: False)\n\n\ndef configure_extension_build():\n    r\"\"\"Configures extension build options according to system environment and user's choice.\n\n    Returns:\n      The input to parameters ext_modules, cmdclass, packages, and entry_points as required in setuptools.setup.\n    \"\"\"\n\n    cmake_cache_vars = get_cmake_cache_vars()\n\n    ################################################################################\n    # Configure compile flags\n    ################################################################################\n\n    library_dirs = []\n    extra_install_requires = []\n\n    if IS_WINDOWS:\n        # /NODEFAULTLIB makes sure we only link to DLL runtime\n        # and matches the flags set for protobuf and ONNX\n        extra_link_args = [\"/NODEFAULTLIB:LIBCMT.LIB\"]\n        # /MD links against DLL runtime\n        # and matches the flags set for protobuf and ONNX\n        # /EHsc is about standard C++ exception handling\n        extra_compile_args = [\"/MD\", \"/FS\", \"/EHsc\"]\n    else:\n        extra_link_args = []\n        extra_compile_args = [\n            \"-Wall\",\n            \"-Wextra\",\n            \"-Wno-strict-overflow\",\n            \"-Wno-unused-parameter\",\n            \"-Wno-missing-field-initializers\",\n            \"-Wno-unknown-pragmas\",\n            # Python 2.6 requires -fno-strict-aliasing, see\n            # http://legacy.python.org/dev/peps/pep-3123/\n            # We also depend on it in our code (even Python 3).\n            \"-fno-strict-aliasing\",\n        ]\n\n    library_dirs.append(lib_path)\n\n    main_compile_args = []\n    main_libraries = [\"torch_python\"]\n    main_link_args = []\n    main_sources = [\"torch/csrc/stub.c\"]\n\n    if cmake_cache_vars[\"USE_CUDA\"]:\n        library_dirs.append(os.path.dirname(cmake_cache_vars[\"CUDA_CUDA_LIB\"]))\n\n    if build_type.is_debug():\n        if IS_WINDOWS:\n            extra_compile_args.append(\"/Z7\")\n            extra_link_args.append(\"/DEBUG:FULL\")\n        else:\n            extra_compile_args += [\"-O0\", \"-g\"]\n            extra_link_args += [\"-O0\", \"-g\"]\n\n    if build_type.is_rel_with_deb_info():\n        if IS_WINDOWS:\n            extra_compile_args.append(\"/Z7\")\n            extra_link_args.append(\"/DEBUG:FULL\")\n        else:\n            extra_compile_args += [\"-g\"]\n            extra_link_args += [\"-g\"]\n\n    # pypi cuda package that requires installation of cuda runtime, cudnn and cublas\n    # should be included in all wheels uploaded to pypi\n    pytorch_extra_install_requirements = os.getenv(\n        \"PYTORCH_EXTRA_INSTALL_REQUIREMENTS\", \"\"\n    )\n    if pytorch_extra_install_requirements:\n        report(\n            f\"pytorch_extra_install_requirements: {pytorch_extra_install_requirements}\"\n        )\n        extra_install_requires += pytorch_extra_install_requirements.split(\"|\")\n\n    # Cross-compile for M1\n    if IS_DARWIN:\n        macos_target_arch = os.getenv(\"CMAKE_OSX_ARCHITECTURES\", \"\")\n        if macos_target_arch in [\"arm64\", \"x86_64\"]:\n            macos_sysroot_path = os.getenv(\"CMAKE_OSX_SYSROOT\")\n            if macos_sysroot_path is None:\n                macos_sysroot_path = (\n                    subprocess.check_output(\n                        [\"xcrun\", \"--show-sdk-path\", \"--sdk\", \"macosx\"]\n                    )\n                    .decode(\"utf-8\")\n                    .strip()\n                )\n            extra_compile_args += [\n                \"-arch\",\n                macos_target_arch,\n                \"-isysroot\",\n                macos_sysroot_path,\n            ]\n            extra_link_args += [\"-arch\", macos_target_arch]\n\n    def make_relative_rpath_args(path):\n        if IS_DARWIN:\n            return [\"-Wl,-rpath,@loader_path/\" + path]\n        elif IS_WINDOWS:\n            return []\n        else:\n            return [\"-Wl,-rpath,$ORIGIN/\" + path]\n\n    ################################################################################\n    # Declare extensions and package\n    ################################################################################\n\n    extensions = []\n    excludes = [\"tools\", \"tools.*\"]\n    if not cmake_cache_vars[\"BUILD_CAFFE2\"]:\n        excludes.extend([\"caffe2\", \"caffe2.*\"])\n    if not cmake_cache_vars[\"BUILD_FUNCTORCH\"]:\n        excludes.extend([\"functorch\", \"functorch.*\"])\n    packages = find_packages(exclude=excludes)\n    C = Extension(\n        \"torch._C\",\n        libraries=main_libraries,\n        sources=main_sources,\n        language=\"c\",\n        extra_compile_args=main_compile_args + extra_compile_args,\n        include_dirs=[],\n        library_dirs=library_dirs,\n        extra_link_args=extra_link_args\n        + main_link_args\n        + make_relative_rpath_args(\"lib\"),\n    )\n    extensions.append(C)\n\n    # These extensions are built by cmake and copied manually in build_extensions()\n    # inside the build_ext implementation\n    if cmake_cache_vars[\"BUILD_CAFFE2\"]:\n        extensions.append(\n            Extension(name=\"caffe2.python.caffe2_pybind11_state\", sources=[]),\n        )\n        if cmake_cache_vars[\"USE_CUDA\"]:\n            extensions.append(\n                Extension(name=\"caffe2.python.caffe2_pybind11_state_gpu\", sources=[]),\n            )\n        if cmake_cache_vars[\"USE_ROCM\"]:\n            extensions.append(\n                Extension(name=\"caffe2.python.caffe2_pybind11_state_hip\", sources=[]),\n            )\n    if cmake_cache_vars[\"BUILD_FUNCTORCH\"]:\n        extensions.append(\n            Extension(name=\"functorch._C\", sources=[]),\n        )\n\n    cmdclass = {\n        \"bdist_wheel\": wheel_concatenate,\n        \"build_ext\": build_ext,\n        \"clean\": clean,\n        \"install\": install,\n        \"sdist\": sdist,\n    }\n\n    entry_points = {\n        \"console_scripts\": [\n            \"convert-caffe2-to-onnx = caffe2.python.onnx.bin.conversion:caffe2_to_onnx\",\n            \"convert-onnx-to-caffe2 = caffe2.python.onnx.bin.conversion:onnx_to_caffe2\",\n            \"torchrun = torch.distributed.run:main\",\n        ]\n    }\n\n    return extensions, cmdclass, packages, entry_points, extra_install_requires\n\n\ndef add_triton(install_requires, extras_require) -> None:\n    \"\"\"\n    Add triton package as a dependency when it's needed\n    \"\"\"\n    # NB: If the installation requirments list already includes triton dependency,\n    # there is no need to add it one more time as an extra dependency. In nightly\n    # or when release PyTorch, that is done by setting PYTORCH_EXTRA_INSTALL_REQUIREMENTS\n    # environment variable on pytorch/builder\n    has_triton = any(\"triton\" in pkg for pkg in install_requires)\n    if has_triton:\n        return\n\n    cmake_cache_vars = get_cmake_cache_vars()\n    use_rocm = cmake_cache_vars[\"USE_ROCM\"]\n    use_cuda = cmake_cache_vars[\"USE_CUDA\"]\n\n    # Triton is only needed for CUDA or ROCm\n    if not use_rocm and not use_cuda:\n        return\n\n    if use_rocm:\n        triton_text_file = \"triton-rocm.txt\"\n        triton_package_name = \"pytorch-triton-rocm\"\n    else:\n        triton_text_file = \"triton.txt\"\n        triton_package_name = \"pytorch-triton\"\n    triton_pin_file = os.path.join(\n        cwd, \".ci\", \"docker\", \"ci_commit_pins\", triton_text_file\n    )\n    triton_version_file = os.path.join(cwd, \".ci\", \"docker\", \"triton_version.txt\")\n\n    if os.path.exists(triton_pin_file) and os.path.exists(triton_version_file):\n        with open(triton_pin_file) as f:\n            triton_pin = f.read().strip()\n        with open(triton_version_file) as f:\n            triton_version = f.read().strip()\n\n        if \"dynamo\" not in extras_require:\n            extras_require[\"dynamo\"] = []\n        extras_require[\"dynamo\"].append(\n            triton_package_name + \"==\" + triton_version + \"+\" + triton_pin[:10]\n        )\n\n\n# post run, warnings, printed at the end to make them more visible\nbuild_update_message = \"\"\"\n    It is no longer necessary to use the 'build' or 'rebuild' targets\n\n    To install:\n      $ python setup.py install\n    To develop locally:\n      $ python setup.py develop\n    To force cmake to re-generate native build files (off by default):\n      $ python setup.py develop --cmake\n\"\"\"\n\n\ndef print_box(msg):\n    lines = msg.split(\"\\n\")\n    size = max(len(l) + 1 for l in lines)\n    print(\"-\" * (size + 2))\n    for l in lines:\n        print(\"|{}{}|\".format(l, \" \" * (size - len(l))))\n    print(\"-\" * (size + 2))\n\n\ndef main():\n    # the list of runtime dependencies required by this built package\n    install_requires = [\n        \"filelock\",\n        \"typing-extensions\",\n        \"sympy\",\n        \"networkx\",\n        \"jinja2\",\n        \"fsspec\",\n    ]\n\n    # Parse the command line and check the arguments before we proceed with\n    # building deps and setup. We need to set values so `--help` works.\n    dist = Distribution()\n    dist.script_name = os.path.basename(sys.argv[0])\n    dist.script_args = sys.argv[1:]\n    try:\n        dist.parse_command_line()\n    except setuptools.distutils.errors.DistutilsArgError as e:\n        print(e)\n        sys.exit(1)\n\n    mirror_files_into_torchgen()\n    if RUN_BUILD_DEPS:\n        build_deps()\n\n    (\n        extensions,\n        cmdclass,\n        packages,\n        entry_points,\n        extra_install_requires,\n    ) = configure_extension_build()\n\n    install_requires += extra_install_requires\n\n    extras_require = {\n        \"optree\": [\"optree>=0.9.1\"],\n        \"opt-einsum\": [\"opt-einsum>=3.3\"],\n    }\n    # Triton is only available on Linux atm\n    if platform.system() == \"Linux\":\n        extras_require[\"dynamo\"] = [\"jinja2\"]\n        add_triton(install_requires=install_requires, extras_require=extras_require)\n\n    # Read in README.md for our long_description\n    with open(os.path.join(cwd, \"README.md\"), encoding=\"utf-8\") as f:\n        long_description = f.read()\n\n    version_range_max = max(sys.version_info[1], 10) + 1\n    torch_package_data = [\n        \"py.typed\",\n        \"bin/*\",\n        \"test/*\",\n        \"*.pyi\",\n        \"_C/*.pyi\",\n        \"cuda/*.pyi\",\n        \"fx/*.pyi\",\n        \"optim/*.pyi\",\n        \"autograd/*.pyi\",\n        \"nn/*.pyi\",\n        \"nn/modules/*.pyi\",\n        \"nn/parallel/*.pyi\",\n        \"utils/data/*.pyi\",\n        \"utils/data/datapipes/*.pyi\",\n        \"lib/*.so*\",\n        \"lib/*.dylib*\",\n        \"lib/*.dll\",\n        \"lib/*.lib\",\n        \"lib/*.pdb\",\n        \"lib/torch_shm_manager\",\n        \"lib/*.h\",\n        \"include/*.h\",\n        \"include/ATen/*.h\",\n        \"include/ATen/cpu/*.h\",\n        \"include/ATen/cpu/vec/vec256/*.h\",\n        \"include/ATen/cpu/vec/vec256/vsx/*.h\",\n        \"include/ATen/cpu/vec/vec256/zarch/*.h\",\n        \"include/ATen/cpu/vec/vec512/*.h\",\n        \"include/ATen/cpu/vec/*.h\",\n        \"include/ATen/core/*.h\",\n        \"include/ATen/cuda/*.cuh\",\n        \"include/ATen/cuda/*.h\",\n        \"include/ATen/cuda/detail/*.cuh\",\n        \"include/ATen/cuda/detail/*.h\",\n        \"include/ATen/cudnn/*.h\",\n        \"include/ATen/functorch/*.h\",\n        \"include/ATen/ops/*.h\",\n        \"include/ATen/hip/*.cuh\",\n        \"include/ATen/hip/*.h\",\n        \"include/ATen/hip/detail/*.cuh\",\n        \"include/ATen/hip/detail/*.h\",\n        \"include/ATen/hip/impl/*.h\",\n        \"include/ATen/mps/*.h\",\n        \"include/ATen/miopen/*.h\",\n        \"include/ATen/detail/*.h\",\n        \"include/ATen/native/*.h\",\n        \"include/ATen/native/cpu/*.h\",\n        \"include/ATen/native/cuda/*.h\",\n        \"include/ATen/native/cuda/*.cuh\",\n        \"include/ATen/native/hip/*.h\",\n        \"include/ATen/native/hip/*.cuh\",\n        \"include/ATen/native/mps/*.h\",\n        \"include/ATen/native/quantized/*.h\",\n        \"include/ATen/native/quantized/cpu/*.h\",\n        \"include/ATen/native/utils/*.h\",\n        \"include/ATen/quantized/*.h\",\n        \"include/caffe2/serialize/*.h\",\n        \"include/c10/*.h\",\n        \"include/c10/macros/*.h\",\n        \"include/c10/core/*.h\",\n        \"include/ATen/core/boxing/*.h\",\n        \"include/ATen/core/boxing/impl/*.h\",\n        \"include/ATen/core/dispatch/*.h\",\n        \"include/ATen/core/op_registration/*.h\",\n        \"include/c10/core/impl/*.h\",\n        \"include/c10/core/impl/cow/*.h\",\n        \"include/c10/util/*.h\",\n        \"include/c10/cuda/*.h\",\n        \"include/c10/cuda/impl/*.h\",\n        \"include/c10/hip/*.h\",\n        \"include/c10/hip/impl/*.h\",\n        \"include/torch/*.h\",\n        \"include/torch/csrc/*.h\",\n        \"include/torch/csrc/api/include/torch/*.h\",\n        \"include/torch/csrc/api/include/torch/data/*.h\",\n        \"include/torch/csrc/api/include/torch/data/dataloader/*.h\",\n        \"include/torch/csrc/api/include/torch/data/datasets/*.h\",\n        \"include/torch/csrc/api/include/torch/data/detail/*.h\",\n        \"include/torch/csrc/api/include/torch/data/samplers/*.h\",\n        \"include/torch/csrc/api/include/torch/data/transforms/*.h\",\n        \"include/torch/csrc/api/include/torch/detail/*.h\",\n        \"include/torch/csrc/api/include/torch/detail/ordered_dict.h\",\n        \"include/torch/csrc/api/include/torch/nn/*.h\",\n        \"include/torch/csrc/api/include/torch/nn/functional/*.h\",\n        \"include/torch/csrc/api/include/torch/nn/options/*.h\",\n        \"include/torch/csrc/api/include/torch/nn/modules/*.h\",\n        \"include/torch/csrc/api/include/torch/nn/modules/container/*.h\",\n        \"include/torch/csrc/api/include/torch/nn/parallel/*.h\",\n        \"include/torch/csrc/api/include/torch/nn/utils/*.h\",\n        \"include/torch/csrc/api/include/torch/optim/*.h\",\n        \"include/torch/csrc/api/include/torch/optim/schedulers/*.h\",\n        \"include/torch/csrc/api/include/torch/serialize/*.h\",\n        \"include/torch/csrc/autograd/*.h\",\n        \"include/torch/csrc/autograd/functions/*.h\",\n        \"include/torch/csrc/autograd/generated/*.h\",\n        \"include/torch/csrc/autograd/utils/*.h\",\n        \"include/torch/csrc/cuda/*.h\",\n        \"include/torch/csrc/distributed/c10d/*.h\",\n        \"include/torch/csrc/distributed/c10d/*.hpp\",\n        \"include/torch/csrc/distributed/rpc/*.h\",\n        \"include/torch/csrc/distributed/autograd/context/*.h\",\n        \"include/torch/csrc/distributed/autograd/functions/*.h\",\n        \"include/torch/csrc/distributed/autograd/rpc_messages/*.h\",\n        \"include/torch/csrc/dynamo/*.h\",\n        \"include/torch/csrc/inductor/*.h\",\n        \"include/torch/csrc/inductor/aoti_runtime/*.h\",\n        \"include/torch/csrc/inductor/aoti_torch/*.h\",\n        \"include/torch/csrc/inductor/aoti_torch/c/*.h\",\n        \"include/torch/csrc/jit/*.h\",\n        \"include/torch/csrc/jit/backends/*.h\",\n        \"include/torch/csrc/jit/generated/*.h\",\n        \"include/torch/csrc/jit/passes/*.h\",\n        \"include/torch/csrc/jit/passes/quantization/*.h\",\n        \"include/torch/csrc/jit/passes/utils/*.h\",\n        \"include/torch/csrc/jit/runtime/*.h\",\n        \"include/torch/csrc/jit/ir/*.h\",\n        \"include/torch/csrc/jit/frontend/*.h\",\n        \"include/torch/csrc/jit/api/*.h\",\n        \"include/torch/csrc/jit/serialization/*.h\",\n        \"include/torch/csrc/jit/python/*.h\",\n        \"include/torch/csrc/jit/mobile/*.h\",\n        \"include/torch/csrc/jit/testing/*.h\",\n        \"include/torch/csrc/jit/tensorexpr/*.h\",\n        \"include/torch/csrc/jit/tensorexpr/operators/*.h\",\n        \"include/torch/csrc/jit/codegen/cuda/*.h\",\n        \"include/torch/csrc/onnx/*.h\",\n        \"include/torch/csrc/profiler/*.h\",\n        \"include/torch/csrc/profiler/orchestration/*.h\",\n        \"include/torch/csrc/profiler/stubs/*.h\",\n        \"include/torch/csrc/utils/*.h\",\n        \"include/torch/csrc/tensor/*.h\",\n        \"include/torch/csrc/lazy/backend/*.h\",\n        \"include/torch/csrc/lazy/core/*.h\",\n        \"include/torch/csrc/lazy/core/internal_ops/*.h\",\n        \"include/torch/csrc/lazy/core/ops/*.h\",\n        \"include/torch/csrc/lazy/python/python_util.h\",\n        \"include/torch/csrc/lazy/ts_backend/*.h\",\n        \"include/pybind11/*.h\",\n        \"include/pybind11/detail/*.h\",\n        \"include/pybind11/eigen/*.h\",\n        \"include/TH/*.h*\",\n        \"include/TH/generic/*.h*\",\n        \"include/THC/*.cuh\",\n        \"include/THC/*.h*\",\n        \"include/THC/generic/*.h\",\n        \"include/THH/*.cuh\",\n        \"include/THH/*.h*\",\n        \"include/THH/generic/*.h\",\n        \"include/sleef.h\",\n        \"_inductor/codegen/*.h\",\n        \"_inductor/codegen/aoti_runtime/*.cpp\",\n        \"share/cmake/ATen/*.cmake\",\n        \"share/cmake/Caffe2/*.cmake\",\n        \"share/cmake/Caffe2/public/*.cmake\",\n        \"share/cmake/Caffe2/Modules_CUDA_fix/*.cmake\",\n        \"share/cmake/Caffe2/Modules_CUDA_fix/upstream/*.cmake\",\n        \"share/cmake/Caffe2/Modules_CUDA_fix/upstream/FindCUDA/*.cmake\",\n        \"share/cmake/Gloo/*.cmake\",\n        \"share/cmake/Tensorpipe/*.cmake\",\n        \"share/cmake/Torch/*.cmake\",\n        \"utils/benchmark/utils/*.cpp\",\n        \"utils/benchmark/utils/valgrind_wrapper/*.cpp\",\n        \"utils/benchmark/utils/valgrind_wrapper/*.h\",\n        \"utils/model_dump/skeleton.html\",\n        \"utils/model_dump/code.js\",\n        \"utils/model_dump/*.mjs\",\n    ]\n\n    if get_cmake_cache_vars()[\"BUILD_CAFFE2\"]:\n        torch_package_data.extend(\n            [\n                \"include/caffe2/**/*.h\",\n                \"include/caffe2/utils/*.h\",\n                \"include/caffe2/utils/**/*.h\",\n            ]\n        )\n    if get_cmake_cache_vars()[\"USE_TENSORPIPE\"]:\n        torch_package_data.extend(\n            [\n                \"include/tensorpipe/*.h\",\n                \"include/tensorpipe/channel/*.h\",\n                \"include/tensorpipe/channel/basic/*.h\",\n                \"include/tensorpipe/channel/cma/*.h\",\n                \"include/tensorpipe/channel/mpt/*.h\",\n                \"include/tensorpipe/channel/xth/*.h\",\n                \"include/tensorpipe/common/*.h\",\n                \"include/tensorpipe/core/*.h\",\n                \"include/tensorpipe/transport/*.h\",\n                \"include/tensorpipe/transport/ibv/*.h\",\n                \"include/tensorpipe/transport/shm/*.h\",\n                \"include/tensorpipe/transport/uv/*.h\",\n            ]\n        )\n    torchgen_package_data = [\n        # Recursive glob doesn't work in setup.py,\n        # https://github.com/pypa/setuptools/issues/1806\n        # To make this robust we should replace it with some code that\n        # returns a list of everything under packaged/\n        \"packaged/ATen/*\",\n        \"packaged/ATen/native/*\",\n        \"packaged/ATen/templates/*\",\n        \"packaged/autograd/*\",\n        \"packaged/autograd/templates/*\",\n    ]\n    setup(\n        name=package_name,\n        version=version,\n        description=(\n            \"Tensors and Dynamic neural networks in \"\n            \"Python with strong GPU acceleration\"\n        ),\n        long_description=long_description,\n        long_description_content_type=\"text/markdown\",\n        ext_modules=extensions,\n        cmdclass=cmdclass,\n        packages=packages,\n        entry_points=entry_points,\n        install_requires=install_requires,\n        extras_require=extras_require,\n        package_data={\n            \"torch\": torch_package_data,\n            \"torchgen\": torchgen_package_data,\n            \"caffe2\": [\n                \"python/serialized_test/data/operator_test/*.zip\",\n            ],\n        },\n        url=\"https://pytorch.org/\",\n        download_url=\"https://github.com/pytorch/pytorch/tags\",\n        author=\"PyTorch Team\",\n        author_email=\"packages@pytorch.org\",\n        python_requires=f\">={python_min_version_str}\",\n        # PyPI package information.\n        classifiers=[\n            \"Development Status :: 5 - Production/Stable\",\n            \"Intended Audience :: Developers\",\n            \"Intended Audience :: Education\",\n            \"Intended Audience :: Science/Research\",\n            \"License :: OSI Approved :: BSD License\",\n            \"Topic :: Scientific/Engineering\",\n            \"Topic :: Scientific/Engineering :: Mathematics\",\n            \"Topic :: Scientific/Engineering :: Artificial Intelligence\",\n            \"Topic :: Software Development\",\n            \"Topic :: Software Development :: Libraries\",\n            \"Topic :: Software Development :: Libraries :: Python Modules\",\n            \"Programming Language :: C++\",\n            \"Programming Language :: Python :: 3\",\n        ]\n        + [\n            f\"Programming Language :: Python :: 3.{i}\"\n            for i in range(python_min_version[1], version_range_max)\n        ],\n        license=\"BSD-3\",\n        keywords=\"pytorch, machine learning\",\n    )\n    if EMIT_BUILD_WARNING:\n        print_box(build_update_message)\n\n\nif __name__ == \"__main__\":\n    main()\n"
    },
    {
        "repository": "celery/celery",
        "file_name": "setup.py",
        "content": "#!/usr/bin/env python3\nimport codecs\nimport os\nimport re\n\nimport setuptools\nimport setuptools.command.test\n\nNAME = 'celery'\n\n# -*- Extras -*-\n\nEXTENSIONS = {\n    'arangodb',\n    'auth',\n    'azureblockblob',\n    'brotli',\n    'cassandra',\n    'consul',\n    'cosmosdbsql',\n    'couchbase',\n    'couchdb',\n    'django',\n    'dynamodb',\n    'elasticsearch',\n    'eventlet',\n    'gevent',\n    'librabbitmq',\n    'memcache',\n    'mongodb',\n    'msgpack',\n    'pymemcache',\n    'pyro',\n    'pytest',\n    'redis',\n    's3',\n    'slmq',\n    'solar',\n    'sqlalchemy',\n    'sqs',\n    'tblib',\n    'yaml',\n    'zookeeper',\n    'zstd'\n}\n\n# -*- Distribution Meta -*-\n\nre_meta = re.compile(r'__(\\w+?)__\\s*=\\s*(.*)')\nre_doc = re.compile(r'^\"\"\"(.+?)\"\"\"')\n\n\ndef _add_default(m):\n    attr_name, attr_value = m.groups()\n    return ((attr_name, attr_value.strip(\"\\\"'\")),)\n\n\ndef _add_doc(m):\n    return (('doc', m.groups()[0]),)\n\n\ndef parse_dist_meta():\n    \"\"\"Extract metadata information from ``$dist/__init__.py``.\"\"\"\n    pats = {re_meta: _add_default, re_doc: _add_doc}\n    here = os.path.abspath(os.path.dirname(__file__))\n    with open(os.path.join(here, NAME, '__init__.py')) as meta_fh:\n        distmeta = {}\n        for line in meta_fh:\n            if line.strip() == '# -eof meta-':\n                break\n            for pattern, handler in pats.items():\n                m = pattern.match(line.strip())\n                if m:\n                    distmeta.update(handler(m))\n        return distmeta\n\n# -*- Requirements -*-\n\n\ndef _strip_comments(l):\n    return l.split('#', 1)[0].strip()\n\n\ndef _pip_requirement(req):\n    if req.startswith('-r '):\n        _, path = req.split()\n        return reqs(*path.split('/'))\n    return [req]\n\n\ndef _reqs(*f):\n    return [\n        _pip_requirement(r) for r in (\n            _strip_comments(l) for l in open(\n                os.path.join(os.getcwd(), 'requirements', *f)).readlines()\n        ) if r]\n\n\ndef reqs(*f):\n    \"\"\"Parse requirement file.\n\n    Example:\n        reqs('default.txt')          # requirements/default.txt\n        reqs('extras', 'redis.txt')  # requirements/extras/redis.txt\n    Returns:\n        List[str]: list of requirements specified in the file.\n    \"\"\"\n    return [req for subreq in _reqs(*f) for req in subreq]\n\n\ndef extras(*p):\n    \"\"\"Parse requirement in the requirements/extras/ directory.\"\"\"\n    return reqs('extras', *p)\n\n\ndef install_requires():\n    \"\"\"Get list of requirements required for installation.\"\"\"\n    return reqs('default.txt')\n\n\ndef extras_require():\n    \"\"\"Get map of all extra requirements.\"\"\"\n    return {x: extras(x + '.txt') for x in EXTENSIONS}\n\n# -*- Long Description -*-\n\n\ndef long_description():\n    try:\n        return codecs.open('README.rst', 'r', 'utf-8').read()\n    except OSError:\n        return 'Long description error: Missing README.rst file'\n\n\nmeta = parse_dist_meta()\nsetuptools.setup(\n    name=NAME,\n    packages=setuptools.find_packages(exclude=['t', 't.*']),\n    version=meta['version'],\n    description=meta['doc'],\n    long_description=long_description(),\n    keywords=meta['keywords'],\n    author=meta['author'],\n    author_email=meta['contact'],\n    url=meta['homepage'],\n    license='BSD-3-Clause',\n    platforms=['any'],\n    install_requires=install_requires(),\n    python_requires=\">=3.8\",\n    tests_require=reqs('test.txt'),\n    extras_require=extras_require(),\n    include_package_data=True,\n    entry_points={\n        'console_scripts': [\n            'celery = celery.__main__:main',\n        ]\n    },\n    project_urls={\n        \"Documentation\": \"https://docs.celeryq.dev/en/stable/\",\n        \"Changelog\": \"https://docs.celeryq.dev/en/stable/changelog.html\",\n        \"Code\": \"https://github.com/celery/celery\",\n        \"Tracker\": \"https://github.com/celery/celery/issues\",\n        \"Funding\": \"https://opencollective.com/celery\"\n    },\n    classifiers=[\n        \"Development Status :: 5 - Production/Stable\",\n        \"License :: OSI Approved :: BSD License\",\n        \"Topic :: System :: Distributed Computing\",\n        \"Topic :: Software Development :: Object Brokering\",\n        \"Framework :: Celery\",\n        \"Programming Language :: Python\",\n        \"Programming Language :: Python :: 3 :: Only\",\n        \"Programming Language :: Python :: 3.8\",\n        \"Programming Language :: Python :: 3.9\",\n        \"Programming Language :: Python :: 3.10\",\n        \"Programming Language :: Python :: 3.11\",\n        \"Programming Language :: Python :: 3.12\",\n        \"Programming Language :: Python :: Implementation :: CPython\",\n        \"Programming Language :: Python :: Implementation :: PyPy\",\n        \"Operating System :: OS Independent\"\n    ]\n)\n"
    }
]
